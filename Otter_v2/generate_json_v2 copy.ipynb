{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ•ã‚©ãƒ«ãƒ€åã®å¤‰æ›´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’\"scratched wood\" â†’ \"scratch\"ã«ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œ<br>\n",
    "â€» å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç›´æ¥æ›¸ãè¾¼ã‚€ã®ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãŠãã“ã¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‰¯å“ ã€€-> None<br>\n",
    "ä¸è‰¯å“ -> æ¬ é™¥å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰‹ä½œæ¥­ãŒå¿…è¦ãªãƒ•ã‚©ãƒ«ãƒ€<br>\n",
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦å‡ºã¦ããŸãƒ•ã‚©ãƒ«ãƒ€ã¯ç›´æ¥ã€å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ¬ é™¥åã«ç·¨é›† or \"broken_chip\" â†’ \"broken_chi\"ãªã©ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹\n",
    "base_folder = \"/data/dataset/yyama_dataset/VI_ICL_images\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    try:\n",
    "        shutil.rmtree(folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼š{e}\")\n",
    "\n",
    "def count_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "if os.path.isdir(base_folder):  # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    for parent_folder in os.listdir(base_folder):\n",
    "        parent_folder_path = os.path.join(base_folder, parent_folder)\n",
    "        if os.path.isdir(parent_folder_path):  # parent_folder_pathãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "            for child_folder in os.listdir(parent_folder_path):\n",
    "                child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "                if os.path.isdir(child_folder_path):  # child_folder_pathãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "                    num_images = count_image_files(child_folder_path)\n",
    "                    if num_images == 0:\n",
    "                        print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "                        delete_folder(child_folder_path)\n",
    "            if len(os.listdir(parent_folder_path)) == 0:\n",
    "                print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {parent_folder_path}\")\n",
    "                delete_folder(parent_folder_path)\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèª\n",
    "if os.path.isdir(base_folder):\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "    for folder_name in os.listdir(base_folder):\n",
    "        subfolder_path = os.path.join(base_folder, folder_name)\n",
    "        \n",
    "        if os.path.isdir(subfolder_path):  # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "            # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "            for subfolder_name in os.listdir(subfolder_path):\n",
    "                child_folder_path = os.path.join(subfolder_path, subfolder_name)\n",
    "                \n",
    "                if os.path.isdir(child_folder_path):  # å­ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                    # å­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "                    files = os.listdir(child_folder_path)\n",
    "                    \n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ•°å­—éƒ¨åˆ†ã‚’å–å¾—ã—ã€ãƒªã‚¹ãƒˆã«ä¿å­˜\n",
    "                    numbers = [int(re.search(r'\\d+', file).group()) for file in files if re.search(r'\\d+', file)]\n",
    "                    \n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›´\n",
    "                    for file in files:\n",
    "                        match = re.search(r'\\d+', file)\n",
    "                        if match:\n",
    "                            number = int(match.group())\n",
    "                            \n",
    "                            # åŒã˜ç•ªå·ãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "                            if numbers.count(number) > 1:\n",
    "                                new_number = 0\n",
    "                                \n",
    "                                # é‡è¤‡ã—ã¦ã„ãªã„ç•ªå·ã‚’æ¢ã™\n",
    "                                while new_number in numbers:\n",
    "                                    new_number += 1\n",
    "                                \n",
    "                                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ\n",
    "                                new_file = file.replace(str(number), str(new_number))\n",
    "                                new_file_path = os.path.join(child_folder_path, new_file)\n",
    "                                \n",
    "                                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´\n",
    "                                os.rename(\n",
    "                                    os.path.join(child_folder_path, file), \n",
    "                                    new_file_path\n",
    "                                )\n",
    "                                \n",
    "                                # ãƒ•ã‚¡ã‚¤ãƒ«åã®å¤‰æ›´ã‚’è¡¨ç¤º\n",
    "                                print(f'{child_folder_path}/{file} -> {new_file_path}')\n",
    "                                \n",
    "                                # ãƒªã‚¹ãƒˆã‚’æ›´æ–°\n",
    "                                numbers.remove(number)\n",
    "                                numbers.append(new_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¬ é™¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\n",
    "anomalies = [\n",
    "    \"None\",\n",
    "    \"torn\",\n",
    "    \"scratch\",\n",
    "    \"hole\",\n",
    "    \"stain\",\n",
    "    \"burn\",\n",
    "    \"crack\",\n",
    "    \"dent\",\n",
    "    \"chip\",\n",
    "    \"warp\",\n",
    "    \"discolor\",\n",
    "    \"rust\",\n",
    "    \"corrosion\",\n",
    "    \"peeling\",\n",
    "    \"bubble\",\n",
    "    \"blister\",\n",
    "    \"tear\",\n",
    "    \"cut\",\n",
    "    \"abrasion\",\n",
    "    \"scuff\",\n",
    "    \"puncture\",\n",
    "    \"fracture\",\n",
    "    \"crease\",\n",
    "    \"deformation\",\n",
    "    \"mold\",\n",
    "    \"mildew\",\n",
    "    \"splinter\",\n",
    "    \"gouge\",\n",
    "    \"bend\",\n",
    "    \"fading\",\n",
    "    \"scorch\",\n",
    "    \"smudge\",\n",
    "    \"mark\",\n",
    "    \"splotch\",\n",
    "    \"grime\",\n",
    "    \"erosion\",\n",
    "    \"smear\",\n",
    "    \"blemish\",\n",
    "    \"bruise\",\n",
    "    \"pitting\",\n",
    "    \"chipping\",\n",
    "    \"delamination\",\n",
    "    \"flaking\",\n",
    "    \"curling\",\n",
    "    \"warping\",\n",
    "    \"twist\",\n",
    "    \"knot\",\n",
    "    \"split\",\n",
    "    \"misalignment\",\n",
    "    \"bulge\",\n",
    "    \"dimple\",\n",
    "    \"waviness\",\n",
    "    \"loose thread\",\n",
    "    \"snag\",\n",
    "    \"dye bleed\",\n",
    "    \"disintegration\",\n",
    "    \"loose seam\",\n",
    "    \"crazing\",\n",
    "    \"contamination\",\n",
    "    \"spalling\",\n",
    "    \"efflorescence\",\n",
    "    \"scaling\",\n",
    "    \"etching\",\n",
    "    \"streak\",\n",
    "    \"dirt\",\n",
    "    \"ravel\",\n",
    "    \"wrinkle\",\n",
    "    \"tatter\",\n",
    "    \"sagging\",\n",
    "    \"drooping\",\n",
    "    \"stretching\",\n",
    "    \"shrinkage\",\n",
    "    \"wobble\",\n",
    "    \"imbalance\",\n",
    "    \"roughness\",\n",
    "    \"warpage\",\n",
    "    \"inclusion\",\n",
    "    \"speck\",\n",
    "    \"pinhole\",\n",
    "    \"adhesive failure\",\n",
    "    \"oxidation\",\n",
    "    \"wear\",\n",
    "    \"fretting\",\n",
    "    \"fatigue\",\n",
    "    \"spatter\",\n",
    "    \"loose connection\",\n",
    "    \"crystallization\",\n",
    "    \"carbon buildup\",\n",
    "    \"tarnish\",\n",
    "    \"uneven surface\",\n",
    "    \"miscoloration\",\n",
    "    \"loose wiring\",\n",
    "    \"moisture damage\",\n",
    "    \"loose bolt\",\n",
    "    \"corroded\",\n",
    "    \"swollen\",\n",
    "    \"tear\",\n",
    "    \"corrosion\",\n",
    "    \"leakage\",\n",
    "    \"swelling\",\n",
    "    \"faded\",\n",
    "    \"fading\",\n",
    "    \"bent\",\n",
    "    \"broken\",\n",
    "    \"aged\",\n",
    "    \"aging\",\n",
    "    \"dog-eared\",\n",
    "    \"contaminant\",\n",
    "    \"contamination\",\n",
    "    \"moss\",\n",
    "    \"oxidized\",\n",
    "    \"oxidation\",\n",
    "    \"loose\",\n",
    "    \"melted\",\n",
    "    \"frayed\",\n",
    "    \"fog\",\n",
    "    \"worn\",\n",
    "    \"wear\",\n",
    "    \"cloudy\",\n",
    "    \"clouding\",\n",
    "    \"dead pixel\",\n",
    "    \"blunt\",\n",
    "    \"bluntness\",\n",
    "    \"peeled\",\n",
    "    \"spot\",\n",
    "    \"missing teeth\",\n",
    "    \"crumpled\",\n",
    "    \"bulging\",\n",
    "    \"clog\",\n",
    "    \"kink\",\n",
    "    \"leak\",\n",
    "    \"insect-eaten\",\n",
    "    \"rot\",\n",
    "    \"wilt\",\n",
    "    \"detached\",\n",
    "    \"wormy\",\n",
    "    \"wormhole\",\n",
    "    \"leaning\",\n",
    "    \"bleach\",\n",
    "    \"unevenly\",\n",
    "    \"unevenness\",\n",
    "    \"uneven\",\n",
    "    \"pitted\",\n",
    "    \"pitting\",\n",
    "    \"crushed\",\n",
    "    \"misprint\",\n",
    "    \"missing\",\n",
    "    \"misaligned\",\n",
    "    \"misalignment\",\n",
    "    \"color bleed\",\n",
    "    \"water damage\",\n",
    "    \"wet\",\n",
    "    \"brittle\",\n",
    "    \"brittleness\",\n",
    "    \"snapped\",\n",
    "    \"misshapen\",\n",
    "    \"wobbly\",\n",
    "    \"Instability\",\n",
    "    \"pilled\",\n",
    "    \"pilling\",\n",
    "    \"eroded\",\n",
    "    \"erosion\",\n",
    "    \"contaminated\",\n",
    "    \"deformed\",\n",
    "    \"shrunken\",\n",
    "    \"shrinkage\",\n",
    "    \"exposed\",\n",
    "    \"blown\",\n",
    "    \"corroded\",\n",
    "    \"stripped\",\n",
    "    \"pills\",\n",
    "    \"pilling\",\n",
    "    \"tangled\",\n",
    "    \"damp\",\n",
    "    \"dampness\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# æ¬ é™¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã¨éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°\n",
    "def check_anomaly(word, anomaly_list):\n",
    "    for anomaly in anomaly_list:\n",
    "        if anomaly in word:\n",
    "            return anomaly\n",
    "    return None\n",
    "\n",
    "for parent_folder in os.listdir(base_folder):\n",
    "    parent_path = os.path.join(base_folder, parent_folder)\n",
    "    \n",
    "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚Œã°å‡¦ç†ã‚’è¡Œã†\n",
    "    if os.path.isdir(parent_path):\n",
    "        \n",
    "        # æ¬ é™¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã¨éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "        matched_anomaly = check_anomaly(parent_folder.replace('_', ' '), anomalies)\n",
    "        \n",
    "        if matched_anomaly:\n",
    "            print(f\"Anomaly '{matched_anomaly}' found in parent folder '{parent_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰‹ä½œæ¥­ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å®Œäº†ã•ã›ãŸã‚ã¨ã«ã€ä»¥ä¸‹è‡ªå‹•ä½œæ¥­ã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# æ¬ é™¥åãƒªã‚¹ãƒˆã¨éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°\n",
    "def check_anomaly(word, anomaly_list, exception_dict):\n",
    "    word = word.lower()\n",
    "    for anomaly in anomaly_list:\n",
    "        if anomaly.lower() in word:\n",
    "            return exception_dict.get(anomaly, anomaly)\n",
    "    return None\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€åã‚’å¤‰æ›´ã™ã‚‹é–¢æ•°\n",
    "def rename_folders(base_path, anomalies, exception_dict):\n",
    "    for parent_folder in os.listdir(base_path):\n",
    "        parent_path = os.path.join(base_path, parent_folder)\n",
    "        \n",
    "        if os.path.isdir(parent_path):\n",
    "            for sub_folder in os.listdir(parent_path):\n",
    "                sub_path = os.path.join(parent_path, sub_folder)\n",
    "                \n",
    "                if sub_folder == parent_folder:\n",
    "                    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãŒåŒã˜åå‰ã®å ´åˆã€\"None\"ã«ãƒªãƒãƒ¼ãƒ \n",
    "                    new_sub_path = os.path.join(parent_path, \"None\")\n",
    "                    os.rename(sub_path, new_sub_path)\n",
    "                else:\n",
    "                    # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯\n",
    "                    sub_folder_ = sub_folder.replace('_', ' ')\n",
    "                    sub_folder_ = sub_folder_.replace(parent_folder.replace('_', ' '), '')\n",
    "                    matched_anomaly = check_anomaly(sub_folder_, anomalies, exception_dict)\n",
    "                    \n",
    "                    if matched_anomaly:\n",
    "                        # ä¸€è‡´ã—ãŸæ¬ é™¥åã«ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒªãƒãƒ¼ãƒ \n",
    "                        new_sub_path = os.path.join(parent_path, matched_anomaly)\n",
    "                        os.rename(sub_path, new_sub_path)\n",
    "                    else:\n",
    "                        # ä¸€è‡´ã—ãªã„å ´åˆã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’åœæ­¢\n",
    "                        print(f\"Anomaly not found for {sub_folder}. Stopping program.\")\n",
    "                        return\n",
    "\n",
    "# ä¾‹å¤–å‡¦ç†è¾æ›¸ (ä¾‹å¤–ã¯ã‚³ã‚³ã¨ä¸Šã®æ¬ é™¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹)\n",
    "exception_dict = {\n",
    "    \"discolor\": \"discoloration\",\n",
    "    \"torn\": \"tear\",\n",
    "    \"corroded\": \"corrosion\",\n",
    "    \"swollen\": \"swelling\",\n",
    "    \"faded\": \"fading\",\n",
    "    \"aged\": \"aging\",\n",
    "    \"contaminant\": \"contamination\",\n",
    "    \"contaminated\": \"contamination\",\n",
    "    \"oxidized\": \"oxidation\",\n",
    "    \"worn\": \"wear\",\n",
    "    \"cloudy\": \"clouding\",\n",
    "    \"blunt\": \"bluntness\",\n",
    "    \"bulging\": \"bulge\",\n",
    "    \"wormy\": \"wormhole\",\n",
    "    \"unevenly\": \"unevenness\",\n",
    "    \"uneven\": \"unevenness\",\n",
    "    \"pitted\": \"pitting\",\n",
    "    \"misaligned\": \"misalignment\",\n",
    "    \"wet\": \"water damage\",\n",
    "    \"brittle\": \"brittleness\",\n",
    "    \"wobbly\": \"Instability\",\n",
    "    \"pilled\": \"pilling\",\n",
    "    \"eroded\": \"erosion\",\n",
    "    \"deformed\": \"deformation\",\n",
    "    \"shrunken\": \"shrinkage\",\n",
    "    \"corroded\": \"corrosion\",\n",
    "    \"pills\": \"pilling\",\n",
    "    \"damp\": \"dampness\",\n",
    "}\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€åã‚’å¤‰æ›´\n",
    "base_folder = \"/data/dataset/yyama_dataset/VI_ICL_images\"\n",
    "rename_folders(base_folder, anomalies, exception_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICLç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ•ã‚©ãƒ«ãƒ€ã®é¸åˆ¥\n",
    "1. å­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒæšæ•°ãŒ1æšä»¥ä¸‹â†’å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "2. è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ1ä»¥ä¸‹ or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„â†’è¦ªãƒ•ã‚©ãƒ«ãƒ€ã”ã¨å‰Šé™¤\n",
    "\n",
    "â€»ã€€\"scratched wood\"ã®ã‚ˆã†ã«ã€\"è£½å“ï¼‹æ¬ é™¥\"ã®å ´åˆã¨ã€\"scratch\"ã®ã‚ˆã†ã«\"æ¬ é™¥\"ã®å ´åˆã¨ã§ã€ä»¥ä¸‹å…¨ã¦å¤‰æ›´ãŒå¿…è¦ã§ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "def list_folders(path, indent=0):\n",
    "    # æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹å†…ã®ã™ã¹ã¦ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    entries = os.listdir(path)\n",
    "\n",
    "    for entry in entries:\n",
    "        full_path = os.path.join(path, entry)\n",
    "\n",
    "        # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆ\n",
    "        if os.path.isdir(full_path):\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã—ã¦è¡¨ç¤º\n",
    "            print(\"  \" * indent + f\"ğŸ“ {entry}\")\n",
    "            # å†å¸°çš„ã«ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚’æ¢ç´¢\n",
    "            list_folders(full_path, indent + 1)\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’è¡¨ç¤º\n",
    "print(f\"ğŸ“ {base_folder}\")\n",
    "list_folders(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã™ã‚‹é–¢æ•°\n",
    "def delete_folder(folder_path):\n",
    "    try:\n",
    "        shutil.rmtree(folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼š{e}\")\n",
    "\n",
    "# ä¸ãˆã‚‰ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ•°ãˆã‚‹é–¢æ•°\n",
    "def count_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’è¿½åŠ \n",
    "    image_count = 0\n",
    "    if os.path.isdir(folder_path):  # ã“ã®è¡Œã‚’è¿½åŠ \n",
    "        for file in os.listdir(folder_path):\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_count += 1\n",
    "    return image_count\n",
    "\n",
    "# è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨åŒåã®å­ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹é–¢æ•°\n",
    "def has_same_name_subfolder(parent_folder_path):\n",
    "    parent_folder_name = os.path.basename(parent_folder_path)\n",
    "    for child_folder in os.listdir(parent_folder_path):\n",
    "        child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "        if os.path.isdir(child_folder_path) and child_folder == parent_folder_name:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "cnt=0\n",
    "for parent_folder in os.listdir(base_folder):\n",
    "    parent_folder_path = os.path.join(base_folder, parent_folder)\n",
    "    if os.path.isdir(parent_folder_path):  # ã“ã®è¡Œã‚’è¿½åŠ \n",
    "        for child_folder in os.listdir(parent_folder_path):\n",
    "            child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "            if os.path.isdir(child_folder_path):  # ã“ã®è¡Œã‚’è¿½åŠ \n",
    "                num_images = count_image_files(child_folder_path)\n",
    "                # ç”»åƒæšæ•°ãŒ2æšæœªæº€ã®å ´åˆã€ãã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "                if num_images < 2:\n",
    "                    print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "                    delete_folder(child_folder_path)\n",
    "        # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ2æœªæº€ or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã€è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "        \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "        if len(os.listdir(parent_folder_path)) < 2 or \"None\" not in os.listdir(parent_folder_path): # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "        # if len(os.listdir(parent_folder_path)) < 2 or not has_same_name_subfolder(parent_folder_path): # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "            print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {parent_folder_path}\")\n",
    "            # cnt+=1\n",
    "            delete_folder(parent_folder_path)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ‹¡å¼µå­ã‚’é™¤ã„ãŸãƒ•ã‚©ãƒ«ãƒ€åã®é‡è¤‡ã‚’ä¿®æ­£<br>\n",
    "punctured_tireå†…ã«\"image_49.jpeg\"ã¨\"image_49.jpg\"ãŒå­˜åœ¨ã—ãŸãŸã‚ã€å…¨ã¦ãƒã‚§ãƒƒã‚¯ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    subfolder_path = os.path.join(base_folder, folder_name)\n",
    "    \n",
    "    # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "    for subfolder_name in os.listdir(subfolder_path):\n",
    "        child_folder_path = os.path.join(subfolder_path, subfolder_name)\n",
    "        \n",
    "        # å­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "        files = os.listdir(child_folder_path)\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ•°å­—éƒ¨åˆ†ã‚’å–å¾—ã—ã€ãƒªã‚¹ãƒˆã«ä¿å­˜\n",
    "        numbers = [int(re.search(r'\\d+', file).group()) for file in files]\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›´\n",
    "        for file in files:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•ªå·ã‚’å–å¾—\n",
    "            number = int(re.search(r'\\d+', file).group())\n",
    "            \n",
    "            # åŒã˜ç•ªå·ãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "            if numbers.count(number) > 1:\n",
    "                new_number = 0\n",
    "                \n",
    "                # é‡è¤‡ã—ã¦ã„ãªã„ç•ªå·ã‚’æ¢ã™\n",
    "                while new_number in numbers:\n",
    "                    new_number += 1\n",
    "                \n",
    "                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ\n",
    "                new_file = file.replace(str(number), str(new_number))\n",
    "                new_file_path = os.path.join(child_folder_path, new_file)\n",
    "                \n",
    "                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´\n",
    "                os.rename(\n",
    "                    os.path.join(child_folder_path, file), \n",
    "                    new_file_path\n",
    "                )\n",
    "                \n",
    "                # ãƒ•ã‚¡ã‚¤ãƒ«åã®å¤‰æ›´ã‚’è¡¨ç¤º\n",
    "                print(f'{child_folder_path}/{file} -> {new_file_path}')\n",
    "                \n",
    "                # ãƒªã‚¹ãƒˆã‚’æ›´æ–°\n",
    "                numbers.remove(number)\n",
    "                numbers.append(new_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainï¼šval = 8ï¼š2 ã«åˆ†ã‘ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "src_dir = base_folder\n",
    "train_dir = \"/data/dataset/yyama_dataset/tasks/VI_ICL/train/\"\n",
    "val_dir = \"/data/dataset/yyama_dataset/tasks/VI_ICL/val/\"\n",
    "\n",
    "# trainã¨valã®å‰²åˆ\n",
    "ratio = 0.8\n",
    "random.seed(42)\n",
    "\n",
    "# trainã¨valã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# src_dirå†…ã®å„ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦\n",
    "for root, dirs, files in os.walk(src_dir):\n",
    "    # ç”»åƒã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    images = [f for f in files if os.path.isfile(os.path.join(root, f))]\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # trainã¨valã«åˆ†å‰²\n",
    "    train_images = images[:int(ratio * len(images))]\n",
    "    val_images = images[int(ratio * len(images)):]\n",
    "    \n",
    "    # trainã¨valã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    train_folder = os.path.join(train_dir, os.path.relpath(root, src_dir))\n",
    "    val_folder = os.path.join(val_dir, os.path.relpath(root, src_dir))\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    \n",
    "    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(root, img), os.path.join(train_folder, img))\n",
    "    for img in val_images:\n",
    "        shutil.copy(os.path.join(root, img), os.path.join(val_folder, img))\n",
    "\n",
    "print('Images copied to train and val folders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valã¨trainã®èª¿æ•´<br>\n",
    "[val]\n",
    "1. ç”»åƒ < 2 ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã¯trainã¸\n",
    "2. å­ãƒ•ã‚©ãƒ«ãƒ€ < 2 or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã€å…¨ã¦trainã¸\n",
    "\n",
    "[train]\n",
    "1. ç”»åƒ < 2 ã®å­ãƒ•ã‚©ãƒ«ãƒ€ â†’ valã‹ã‚‰trainã¸\n",
    "2. å­ãƒ•ã‚©ãƒ«ãƒ€ < 2 or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆ â†’ valã‹ã‚‰å…¨ã¦trainã¸\n",
    "\n",
    "[val] ã‚‚ã†ä¸€åº¦\n",
    "1. ç”»åƒ < 2 ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã¯trainã¸\n",
    "2. å­ãƒ•ã‚©ãƒ«ãƒ€ < 2 or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã€å…¨ã¦trainã¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç§»å‹•ã•ã›ã‚‹é–¢æ•°\n",
    "def move_child_folder(src, dst):\n",
    "    \"\"\"\n",
    "    :param src: ç§»å‹•ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    :param dst: ç§»å‹•å…ˆã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    if os.path.exists(dst):\n",
    "        # ç§»å‹•å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ç§»å‹•\n",
    "        for file_name in os.listdir(src):\n",
    "            full_file_name = os.path.join(src, file_name)\n",
    "            if os.path.isfile(full_file_name):\n",
    "                shutil.move(full_file_name, dst)\n",
    "        # ã‚‚ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "        os.rmdir(src)\n",
    "    else:\n",
    "        # ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ç§»å‹•\n",
    "        if not os.path.exists(os.path.dirname(dst)):\n",
    "            os.makedirs(os.path.dirname(dst))\n",
    "        shutil.move(src, os.path.join(os.path.dirname(dst), os.path.basename(src)))\n",
    "\n",
    "# è¦ªãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ç§»å‹•ã•ã›ã‚‹é–¢æ•°\n",
    "def move_parent_folder(src, dst):\n",
    "    \"\"\"\n",
    "    :param src: ç§»å‹•ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    :param dst: ç§»å‹•å…ˆã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    for src_dir, dirs, files in os.walk(src):\n",
    "        dst_dir = src_dir.replace(src, dst, 1)\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        for file_ in files:\n",
    "            src_file = os.path.join(src_dir, file_)\n",
    "            dst_file = os.path.join(dst_dir, file_)\n",
    "            if os.path.exists(dst_file):\n",
    "                os.remove(dst_file)\n",
    "            shutil.move(src_file, dst_dir)\n",
    "    shutil.rmtree(src)\n",
    "    \n",
    "# ç§»å‹•å…ˆã®ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "def change_path(path, new_base):\n",
    "    # ãƒ‘ã‚¹ã‚’åˆ†å‰²\n",
    "    parts = path.split('/')\n",
    "    # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ã§ãƒ‘ã‚¹ã‚’å†æ§‹ç¯‰\n",
    "    parts[1] = new_base\n",
    "    return '/'.join(parts)\n",
    "\n",
    "# ä¸ãˆã‚‰ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ•°ãˆã‚‹é–¢æ•°\n",
    "def count_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’è¿½åŠ \n",
    "    image_count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "def val(val_dir, val_base):\n",
    "    cnt=0\n",
    "    for parent_folder in os.listdir(val_dir):\n",
    "        parent_folder_path = os.path.join(val_dir, parent_folder)\n",
    "        # print(f'parent_folder_path : {parent_folder_path}')\n",
    "        pf = Path(parent_folder_path).name + \"/\"\n",
    "        for child_folder in os.listdir(parent_folder_path):\n",
    "            child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "            cf = Path(child_folder_path).name\n",
    "            num_images = count_image_files(child_folder_path)\n",
    "            # print(f\"ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "            # ç”»åƒæšæ•°ãŒ2æšæœªæº€ã®å ´åˆã€ãã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’trainã¸ç§»å‹•\n",
    "            if num_images < 2:\n",
    "                cnt+=1\n",
    "                # new_path = change_path(child_folder_path, val_base)\n",
    "                print(f\"ç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, ç§»å‹•å…ˆ: {val_base+pf+cf}, æšæ•°: {num_images}\")\n",
    "                move_child_folder(child_folder_path, val_base+pf+cf)\n",
    "\n",
    "        # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ2æœªæº€ or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã€è¦ªãƒ•ã‚©ãƒ«ãƒ€ã”ã¨trainã¸ç§»å‹•\n",
    "        \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "        if len(os.listdir(parent_folder_path)) < 2 or \"None\" not in os.listdir(parent_folder_path): # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "        # if len(os.listdir(parent_folder_path)) < 2 or not has_same_name_subfolder(parent_folder_path): # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "            cnt+=1\n",
    "            new_path = change_path(parent_folder_path, val_base)\n",
    "            print(f\"asleih;aflwefgh;aç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {parent_folder_path}, ç§»å‹•å…ˆ: {val_base+pf+cf}, {num_images=}\")\n",
    "            \n",
    "            move_parent_folder(parent_folder_path, val_base+pf+cf)\n",
    "    print(cnt)\n",
    "            \n",
    "def train(train_dir, train_base):\n",
    "    cnt=0\n",
    "    for parent_folder in os.listdir(train_dir):\n",
    "        parent_folder_path = os.path.join(train_dir, parent_folder)\n",
    "        pf = Path(parent_folder_path).name + \"/\"\n",
    "        for child_folder in os.listdir(parent_folder_path):\n",
    "            child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "            cf = Path(child_folder_path).name\n",
    "            num_images = count_image_files(child_folder_path)\n",
    "            # print(f\"ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "            # ç”»åƒæšæ•°ãŒ2æšæœªæº€ã®å ´åˆã€ãã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’valã‹ã‚‰ç§»å‹•\n",
    "            if num_images < 2:\n",
    "                cnt+=1\n",
    "                print(f\"ç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}\")\n",
    "                print(f\"ç§»å‹•å…ˆ: {train_base+pf+cf}\")\n",
    "                # child_folder_path = change_path(child_folder_path, train_base)\n",
    "                print(f\"ç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, ç§»å‹•å…ˆ: {train_base+pf+cf}, æšæ•°: {num_images}\")\n",
    "\n",
    "                move_child_folder(child_folder_path, train_base+pf+cf)\n",
    "\n",
    "\n",
    "        # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ2æœªæº€ or è‰¯å“ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„å ´åˆã€è¦ªãƒ•ã‚©ãƒ«ãƒ€ã”ã¨valã‹ã‚‰ç§»å‹•\n",
    "        \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "        if len(os.listdir(parent_folder_path)) < 2 or \"None\" not in os.listdir(parent_folder_path): # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "        # if len(os.listdir(parent_folder_path)) < 2 or not has_same_name_subfolder(parent_folder_path): # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "            cnt+=1\n",
    "            # print(f'new path:{parent_folder_path}')\n",
    "            # new_path = change_path(parent_folder_path, train_base)\n",
    "            # print(f\"?fbgosnvalmsç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {new_path}, ç§»å‹•å…ˆ: { train_base+pf+cf}\")\n",
    "            move_parent_folder(parent_folder_path,  train_base+pf+cf)\n",
    "    print(cnt)\n",
    "\n",
    "train_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train/'\n",
    "train_base = 'data/dataset/yyama_dataset/tasks/VI_ICL/val/'\n",
    "val_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val/'\n",
    "val_base = '/data/dataset/yyama_dataset/tasks/VI_ICL/train/'\n",
    "\n",
    "val(val_dir, val_base)\n",
    "train(train_dir, train_base)\n",
    "val(val_dir, val_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_missing_folders(folder1, folder2):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸ2ã¤ã®ãƒ•ã‚©ãƒ«ãƒ€é–“ã§å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã¦å‡ºåŠ›ã—ã¾ã™ã€‚\n",
    "    Args:\n",
    "        folder1 (str): æœ€åˆã®ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "        folder2 (str): 2ç•ªç›®ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€1å†…ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    folders1 = os.listdir(folder1)\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€2å†…ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    folders2 = os.listdir(folder2)\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€1ã«å­˜åœ¨ã—ã€ãƒ•ã‚©ãƒ«ãƒ€2ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "    missing_folders = [folder for folder in folders1 if folder not in folders2]\n",
    "\n",
    "    # çµæœã‚’å‡ºåŠ›\n",
    "    print(f\"ãƒ•ã‚©ãƒ«ãƒ€ '{folder1}' ã«ã‚ã£ã¦ '{folder2}' ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€:\")\n",
    "    for folder in missing_folders:\n",
    "        print(folder)\n",
    "\n",
    "# ä½¿ç”¨ä¾‹\n",
    "# folder1_path = '/data/yyama_dataset/tasks/SD/train'\n",
    "# folder2_path = '/data/yyama_dataset/tasks/SD/val'\n",
    "folder1_path = '/data/dataset/yyama_dataset/tasks/VI_ICL/train'\n",
    "folder2_path = '/data/dataset/yyama_dataset/tasks/VI_ICL/val'\n",
    "find_missing_folders(folder1_path, folder2_path)\n",
    "find_missing_folders(folder2_path, folder1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context : query = 1 : 1 ã«åˆ†ã‘ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_context_query(src_dir, train_context_dir, train_query_dir):\n",
    "    # train_contextã¨train_queryã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "\n",
    "    os.makedirs(train_context_dir, exist_ok=True)\n",
    "    os.makedirs(train_query_dir, exist_ok=True)\n",
    "\n",
    "    # src_dirå†…ã®å„ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        # ç”»åƒã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "        images = [f for f in files if os.path.isfile(os.path.join(root, f))]\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        # train_contextã¨train_queryã«åˆ†å‰²\n",
    "        train_context_images = images[:len(images)//2]\n",
    "        train_query_images = images[len(images)//2:]\n",
    "        print(f'context:{len(train_context_images)}')\n",
    "        print(f'query:{len(train_query_images)}')\n",
    "        print(f'query : {train_query_images}')\n",
    "        \n",
    "        # train_contextã¨train_queryã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "        train_context_folder = os.path.join(train_context_dir, os.path.relpath(root, src_dir))\n",
    "        train_query_folder = os.path.join(train_query_dir, os.path.relpath(root, src_dir))\n",
    "        print(train_query_folder)\n",
    "        os.makedirs(train_context_folder, exist_ok=True)\n",
    "        os.makedirs(train_query_folder, exist_ok=True)\n",
    "        \n",
    "        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
    "        for img in train_context_images:\n",
    "            shutil.copy(os.path.join(root, img), os.path.join(train_context_folder, img))\n",
    "        for img in train_query_images:\n",
    "            shutil.copy(os.path.join(root, img), os.path.join(train_query_folder, img))\n",
    "\n",
    "    print('Images copied to train_context and train_query folders.')\n",
    "    \n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "# src_dir = '/data/yyama_dataset/tasks/SD/duplicated_data/train'\n",
    "# train_context_dir = '/data/yyama_dataset/tasks/SD/duplicated_data/train_context/'\n",
    "# train_query_dir =   '/data/yyama_dataset/tasks/SD/duplicated_data/train_query/'\n",
    "# split_context_query(src_dir, train_context_dir, train_query_dir)\n",
    "\n",
    "# src_dir = '/data/yyama_dataset/tasks/SD/duplicated_data/val'\n",
    "# train_context_dir = '/data/yyama_dataset/tasks/SD/duplicated_data/val_context/'\n",
    "# train_query_dir = '/data/yyama_dataset/tasks/SD/duplicated_data/val_query/'\n",
    "# split_context_query(src_dir, train_context_dir, train_query_dir)\n",
    "\n",
    "src_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train'\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_query/'\n",
    "split_context_query(src_dir, train_context_dir, train_query_dir)\n",
    "\n",
    "src_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val'\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_query/'\n",
    "split_context_query(src_dir, train_context_dir, train_query_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context, queryã®ä¸€æ–¹ã«ã—ã‹å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã„ã‹ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def diff(dir1, dir2):\n",
    "    # train_contextã¨train_queryã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—\n",
    "    folders1 = [os.path.relpath(root, dir1) for root, _, _ in os.walk(dir1)]\n",
    "    folders2 = [os.path.relpath(root, dir2) for root, _, _ in os.walk(dir2)]\n",
    "\n",
    "    # train_contextã«å­˜åœ¨ã—ã€train_queryã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‡ºåŠ›\n",
    "    unique_folders = set(folders1) - set(folders2)\n",
    "    print(f\"{dir1}ã«å­˜åœ¨ã—ã€{dir2}ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€:\")\n",
    "    for folder in unique_folders:\n",
    "        print(folder)\n",
    "        \n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_query/'\n",
    "\n",
    "diff(train_context_dir, train_query_dir)\n",
    "diff(train_query_dir, train_context_dir)\n",
    "\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_query/'\n",
    "\n",
    "diff(train_context_dir, train_query_dir)\n",
    "diff(train_query_dir, train_context_dir)\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/train_query/'\n",
    "\n",
    "diff(train_context_dir, train_query_dir)\n",
    "diff(train_query_dir, train_context_dir)\n",
    "\n",
    "train_context_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_context/'\n",
    "train_query_dir = '/data/dataset/yyama_dataset/tasks/VI_ICL/val_query/'\n",
    "\n",
    "diff(train_context_dir, train_query_dir)\n",
    "diff(train_query_dir, train_context_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_instructions.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_folder = \"/data/yyama_dataset/tasks/VI_ICL\"  # jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "os.makedirs(base_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å…ˆã«å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’è€ƒæ…®ã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼é–¢æ•°\n",
    "    \"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def generate_json_from_directory(directory_path, output_json_path): # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨\n",
    "    output = {\"data\": {}}\n",
    "    i = 0\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "    for main_folder in os.listdir(directory_path):\n",
    "        main_folder_path = os.path.join(directory_path, main_folder)\n",
    "        \n",
    "        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ã®ç¢ºèª\n",
    "        if os.path.isdir(main_folder_path):\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "            for sub_folder in os.listdir(main_folder_path):\n",
    "                sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "                \n",
    "                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜‡é †ã«èµ°æŸ»\n",
    "                for image_file in sorted(os.listdir(sub_folder_path), key=natural_sort_key):\n",
    "                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’é™¤å»\n",
    "                    image_name_without_extension = os.path.splitext(image_file)[0]\n",
    "                    \n",
    "                    # ã‚­ãƒ¼ã®åå‰ã‚’ç”Ÿæˆ\n",
    "                    key_name = f\"{main_folder}+{sub_folder}+{image_name_without_extension}\"\n",
    "                    \n",
    "                    # JSONã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ\n",
    "                    output[\"data\"][key_name] = {\n",
    "                        \"instruction\": \"\",\n",
    "                        \"answer\": \"\",\n",
    "                        \"flag\": \"context\",\n",
    "                        \"image_ids\": [key_name],\n",
    "                        \"rel_ok_ins_ids\": [],\n",
    "                        \"rel_ng_ins_ids\": [],\n",
    "                        \"label\": i\n",
    "                    }\n",
    "                i += 1\n",
    "    \n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(output, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"/data/yyama_dataset/tasks/VI_ICL/train_instructions.json\"\n",
    "context_path = \"/data/yyama_dataset/tasks/VI_ICL/train_context\"\n",
    "generate_json_from_directory(context_path, output_json_path)\n",
    "\n",
    "output_json_path = \"/data/yyama_dataset/tasks/VI_ICL/val_instructions.json\"\n",
    "context_path = \"/data/yyama_dataset/tasks/VI_ICL/val_context\"\n",
    "generate_json_from_directory(context_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å®Ÿè¡Œã—ãŸå¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’è€ƒæ…®ã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼é–¢æ•°\n",
    "    \"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def generate_and_merge_json_from_directory(directory_path, old_json_path, NUM): #ã‚¯ã‚¨ãƒªç”¨\n",
    "    # ä»¥å‰ã®JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(old_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "\n",
    "    new_data = {\"data\": {}}\n",
    "    i = 0\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "    for main_folder in os.listdir(directory_path):\n",
    "        main_folder_path = os.path.join(directory_path, main_folder)\n",
    "\n",
    "        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ã®ç¢ºèª\n",
    "        if os.path.isdir(main_folder_path):\n",
    "\n",
    "            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "            for sub_folder in os.listdir(main_folder_path):\n",
    "                sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "\n",
    "                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜‡é †ã«èµ°æŸ»\n",
    "                for image_file in sorted(os.listdir(sub_folder_path), key=natural_sort_key):\n",
    "                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’é™¤å»\n",
    "                    image_name_without_extension = os.path.splitext(image_file)[0]\n",
    "\n",
    "                    # ã‚­ãƒ¼ã®åå‰ã‚’ç”Ÿæˆ\n",
    "                    key_name = f\"{main_folder}+{sub_folder}+{image_name_without_extension}\"\n",
    "\n",
    "                    # rel_ok_ins_idsã®å€¤ã‚’å–å¾—\n",
    "                    \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                    related_ok_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+None\")] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                    # related_ok_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+{main_folder}+\")] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                    rel_ok_ins_ids = random.sample(related_ok_keys, min(NUM, len(related_ok_keys)))\n",
    "\n",
    "                    # rel_ng_ins_idsã®å€¤ã‚’å–å¾—\n",
    "                    \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                    # if sub_folder == \"None\": # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                    if main_folder==sub_folder: # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                        \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                        related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+\") and not key.startswith(f\"{main_folder}+None\")] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                        # related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+\") and not key.startswith(f\"{main_folder}+{main_folder}+\")] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                        related_ng_key_lists = {}\n",
    "                        for key in related_ng_keys:\n",
    "                            prefix = key.split('+')[1]\n",
    "                            if prefix not in related_ng_key_lists:\n",
    "                                related_ng_key_lists[prefix] = []\n",
    "                            related_ng_key_lists[prefix].append(key)\n",
    "\n",
    "                        rel_ng_ins_ids = []\n",
    "                        while len(rel_ng_ins_ids) < NUM and related_ng_key_lists:\n",
    "                            for prefix in list(related_ng_key_lists.keys()):\n",
    "                                if related_ng_key_lists[prefix]:\n",
    "                                    selected_key = random.choice(related_ng_key_lists[prefix])\n",
    "                                    rel_ng_ins_ids.append(selected_key)\n",
    "                                    related_ng_key_lists[prefix].remove(selected_key)\n",
    "                                else:\n",
    "                                    del related_ng_key_lists[prefix]\n",
    "                    else: # ä¸è‰¯å“ã®å ´åˆ\n",
    "                        related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+{sub_folder}+\")]\n",
    "                        rel_ng_ins_ids = random.sample(related_ng_keys, min(NUM, len(related_ng_keys)))\n",
    "\n",
    "                    # JSONã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ\n",
    "                    new_data[\"data\"][key_name] = {\n",
    "                        \"instruction\": \"\",\n",
    "                        \"answer\": \"\",\n",
    "                        \"flag\": \"query\",\n",
    "                        \"image_ids\": [key_name],\n",
    "                        \"rel_ok_ins_ids\": rel_ok_ins_ids,\n",
    "                        \"rel_ng_ins_ids\": rel_ng_ins_ids,\n",
    "                        \"label\": i\n",
    "                    }\n",
    "                i += 1\n",
    "\n",
    "    # æ—¢å­˜ã®JSONãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸\n",
    "    old_data[\"data\"].update(new_data[\"data\"])\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    # old_json_path = \"output.json\"\n",
    "    with open(old_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(old_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "query_path = \"./full_images_train_query\"\n",
    "NUM = 5 # è‰¯å“ã€ä¸è‰¯å“ã‚’ãã‚Œãã‚Œ5æšãšã¤é¸ã¶\n",
    "generate_and_merge_json_from_directory(query_path, output_json_path, NUM)\n",
    "\n",
    "output_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "query_path = \"./full_images_val_query\"\n",
    "NUM = 5 # è‰¯å“ã€ä¸è‰¯å“ã‚’ãã‚Œãã‚Œ5æšãšã¤é¸ã¶\n",
    "generate_and_merge_json_from_directory(query_path, output_json_path, NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "def count_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "        \n",
    "    entries = [item for item in data.values() if item[\"flag\"] == \"context\"]\n",
    "    count = len(entries)\n",
    "    print(f\"context: {count}\")\n",
    "        \n",
    "    entries = [item for item in data.values() if item[\"flag\"] == \"query\"]\n",
    "    count = len(entries)\n",
    "    print(f\"query: {count}\")\n",
    "\n",
    "print(\"train\")\n",
    "json_path = f\"{base_folder}/train_instructions.json\"\n",
    "count_data(json_path)\n",
    "\n",
    "print(\"val\")\n",
    "json_path = f\"{base_folder}/val_instructions.json\"\n",
    "count_data(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆå½¢å¼ã®æ–‡å­—åˆ—ã‚’ä½œæˆã™ã‚‹é–¢æ•°\n",
    "def generate_list_string(items):\n",
    "    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    \n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "\n",
    "def fill_instruction_and_answer(json_path, train_context_dir):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key, value in data.items():\n",
    "        # ã‚­ãƒ¼ã®åå‰ã‹ã‚‰è¦ªãƒ•ã‚©ãƒ«ãƒ€åã¨å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’èªè­˜ã™ã‚‹\n",
    "        parent_folder, child_folder, _ = key.split(\"+\")\n",
    "\n",
    "        # \"./1_train_context/è¦ªãƒ•ã‚©ãƒ«ãƒ€å\"ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ãã®ä¸­ã«ã‚ã‚‹å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹\n",
    "        subfolder_list = os.listdir(os.path.join(train_context_dir, parent_folder))\n",
    "        subfolder_list = [folder for folder in subfolder_list if folder != \"None\"] # æ¬ é™¥åã®ã¿ãƒªã‚¹ãƒˆåŒ–\n",
    "\n",
    "        # \"instruction\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        subfolder_string = generate_list_string(subfolder_list)\n",
    "        parent_folder__ = parent_folder.replace('_', ' ')\n",
    "        # long\n",
    "        value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects such as {subfolder_string}?'\n",
    "        # short\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects?'\n",
    "        # latest long\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        # not object name\n",
    "        # value[\"instruction\"] = f'Does this image have any defects? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        # product\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this image have any defects? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        \n",
    "        \n",
    "        # \"answer\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        # if child_folder == \"None\": # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "        if child_folder == parent_folder: # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "            value[\"answer\"] = f'No. This {parent_folder__} does not have any defects such as {subfolder_string}, so it is non-defective.'\n",
    "            # value[\"answer\"] = f'No. This {parent_folder__} does not have any defects, so it is non-defective.'\n",
    "            # value[\"answer\"] = f'No.'\n",
    "            # value[\"answer\"] = f'No None'\n",
    "        else:\n",
    "            child_folder = child_folder.replace('_', ' ')\n",
    "            value[\"answer\"] = f'Yes. This {parent_folder__} has some {child_folder}, so it is defective.'\n",
    "            # value[\"answer\"] = f'Yes.'\n",
    "            # value[\"answer\"] = f'Yes {child_folder}'\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "train_context_dir = \"./full_images_train_context\" # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ok (ã‚¯ã‚¨ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã®ãŸã‚)\n",
    "fill_instruction_and_answer(output_json_path, train_context_dir)\n",
    "\n",
    "output_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "train_context_dir = \"./full_images_val_context\" # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ok (ã‚¯ã‚¨ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã®ãŸã‚)\n",
    "fill_instruction_and_answer(output_json_path, train_context_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_train.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    train_data = {}\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key, value in data.items():\n",
    "        # 'flag'=='query'ã®ã‚­ãƒ¼ã‚’é¸æŠ\n",
    "        if value[\"flag\"] == \"query\":\n",
    "            # \"rel_ok_ins_ids\"ã¨\"rel_ng_ins_ids\"ã‚’èª­ã¿è¾¼ã¿\n",
    "            ok_ids = value[\"rel_ok_ins_ids\"]\n",
    "            ng_ids = value[\"rel_ng_ins_ids\"]\n",
    "\n",
    "            # ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜\n",
    "            combinations = []\n",
    "            for ok_id in ok_ids:\n",
    "                for ng_id in ng_ids:\n",
    "                    combinations.append([ok_id, ng_id])\n",
    "\n",
    "            # ãƒªã‚¹ãƒˆå†…ã®å„çµ„å†…ã®è¦ç´ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            for combination in combinations:\n",
    "                random.shuffle(combination)\n",
    "\n",
    "            # ã‚­ãƒ¼åã‚’æ›´æ–°ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\n",
    "            random.shuffle(combinations)\n",
    "            for i, combination in enumerate(combinations[:NUM_PAIRS]):\n",
    "                train_data[f\"{key}={i}\"] = combination\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(train_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "NUM_PAIRS = 25 # 1ã¤ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹çµ„ã¿åˆã‚ã›ã®æ•°\n",
    "input_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/train_pairs{NUM_PAIRS}_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS)\n",
    "\n",
    "NUM_PAIRS__ = 1 # 1ã¤ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹çµ„ã¿åˆã‚ã›ã®æ•°ï¼ˆvalã¯1ã«ã™ã‚‹ã¨ã‚¯ã‚¨ãƒªãŒä¸€å›ã®ã¿ï¼‰\n",
    "input_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/val_pairs{NUM_PAIRS__}_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "def count_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    count = len(data.keys())\n",
    "    print(f\"ãƒšã‚¢æ•°: {count}\")\n",
    "    # oks = [key for key in data.keys() if key.split(\"+\")[1] == \"None\"] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "    oks = [key for key in data.keys() if key.split(\"+\")[0] == key.split(\"+\")[1]] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "    print(f\"ã‚¯ã‚¨ãƒªè‰¯å“: {len(oks)}\")\n",
    "    # ngs = [key for key in data.keys() if key.split(\"+\")[1] != \"None\"] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "    ngs = [key for key in data.keys() if key.split(\"+\")[0] != key.split(\"+\")[1]] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "    print(f\"ã‚¯ã‚¨ãƒªä¸è‰¯å“: {len(ngs)}\")\n",
    "    \n",
    "print(\"train\")\n",
    "# NUM_PAIRS = 25\n",
    "json_path = f\"{base_folder}/train_pairs{NUM_PAIRS}_train.json\"\n",
    "count_data(json_path)\n",
    "\n",
    "print(\"val\")\n",
    "# NUM_PAIRS__ = 1\n",
    "json_path = f\"{base_folder}/val_pairs{NUM_PAIRS__}_train.json\"\n",
    "count_data(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_instructions.jsonã®ä½œæˆï¼ˆFrame3ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_folder = \"./jsons/frame3/full_images_defect_name_longQ\"  # jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "os.makedirs(base_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å…ˆã«å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’è€ƒæ…®ã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼é–¢æ•°\n",
    "    \"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def generate_json_from_directory(directory_path, output_json_path): # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”¨\n",
    "    output = {\"data\": {}}\n",
    "    i = 0\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "    for main_folder in os.listdir(directory_path):\n",
    "        main_folder_path = os.path.join(directory_path, main_folder)\n",
    "        \n",
    "        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ã®ç¢ºèª\n",
    "        if os.path.isdir(main_folder_path):\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "            for sub_folder in os.listdir(main_folder_path):\n",
    "                sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "                \n",
    "                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜‡é †ã«èµ°æŸ»\n",
    "                for image_file in sorted(os.listdir(sub_folder_path), key=natural_sort_key):\n",
    "                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’é™¤å»\n",
    "                    image_name_without_extension = os.path.splitext(image_file)[0]\n",
    "                    \n",
    "                    # ã‚­ãƒ¼ã®åå‰ã‚’ç”Ÿæˆ\n",
    "                    key_name = f\"{main_folder}+{sub_folder}+{image_name_without_extension}\"\n",
    "                    \n",
    "                    # JSONã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ\n",
    "                    output[\"data\"][key_name] = {\n",
    "                        \"instruction\": \"\",\n",
    "                        \"answer\": \"\",\n",
    "                        \"flag\": \"context\",\n",
    "                        \"image_ids\": [key_name],\n",
    "                        \"rel_ok_ins_ids\": [],\n",
    "                        \"rel_ng_ins_ids\": [],\n",
    "                        \"label\": i\n",
    "                    }\n",
    "                i += 1\n",
    "    \n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(output, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "context_path = \"./VI_full_train_context\"\n",
    "generate_json_from_directory(context_path, output_json_path)\n",
    "\n",
    "output_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "context_path = \"./VI_full_val_context\"\n",
    "generate_json_from_directory(context_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å®Ÿè¡Œã—ãŸå¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’è€ƒæ…®ã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼é–¢æ•°\n",
    "    \"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def generate_and_merge_json_from_directory(directory_path, old_json_path, NUM): #ã‚¯ã‚¨ãƒªç”¨\n",
    "    # ä»¥å‰ã®JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(old_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "\n",
    "    new_data = {\"data\": {}}\n",
    "    i = 0\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "    for main_folder in os.listdir(directory_path):\n",
    "        main_folder_path = os.path.join(directory_path, main_folder)\n",
    "\n",
    "        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ã®ç¢ºèª\n",
    "        if os.path.isdir(main_folder_path):\n",
    "\n",
    "            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "            for sub_folder in os.listdir(main_folder_path):\n",
    "                sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "\n",
    "                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜‡é †ã«èµ°æŸ»\n",
    "                for image_file in sorted(os.listdir(sub_folder_path), key=natural_sort_key):\n",
    "                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’é™¤å»\n",
    "                    image_name_without_extension = os.path.splitext(image_file)[0]\n",
    "\n",
    "                    # ã‚­ãƒ¼ã®åå‰ã‚’ç”Ÿæˆ\n",
    "                    key_name = f\"{main_folder}+{sub_folder}+{image_name_without_extension}\"\n",
    "\n",
    "                    # rel_ok_ins_idsã®å€¤ã‚’å–å¾—\n",
    "                    \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                    # related_ok_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+None\")] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                    related_ok_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+{main_folder}+\")] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                    rel_ok_ins_ids = random.sample(related_ok_keys, min(NUM, len(related_ok_keys)))\n",
    "\n",
    "                    # rel_ng_ins_idsã®å€¤ã‚’å–å¾—\n",
    "                    \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                    # if sub_folder == \"None\": # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                    if main_folder==sub_folder: # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                        \"\"\"ä»¥ä¸‹ã©ã¡ã‚‰ã‹é¸æŠ\"\"\"\n",
    "                        # related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+\") and not key.startswith(f\"{main_folder}+None\")] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "                        related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+\") and not key.startswith(f\"{main_folder}+{main_folder}+\")] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "                        related_ng_key_lists = {}\n",
    "                        for key in related_ng_keys:\n",
    "                            prefix = key.split('+')[1]\n",
    "                            if prefix not in related_ng_key_lists:\n",
    "                                related_ng_key_lists[prefix] = []\n",
    "                            related_ng_key_lists[prefix].append(key)\n",
    "\n",
    "                        rel_ng_ins_ids = []\n",
    "                        while len(rel_ng_ins_ids) < NUM and related_ng_key_lists:\n",
    "                            for prefix in list(related_ng_key_lists.keys()):\n",
    "                                if related_ng_key_lists[prefix]:\n",
    "                                    selected_key = random.choice(related_ng_key_lists[prefix])\n",
    "                                    rel_ng_ins_ids.append(selected_key)\n",
    "                                    related_ng_key_lists[prefix].remove(selected_key)\n",
    "                                else:\n",
    "                                    del related_ng_key_lists[prefix]\n",
    "                    else: # ä¸è‰¯å“ã®å ´åˆ\n",
    "                        related_ng_keys = [key for key in old_data[\"data\"].keys() if key.startswith(f\"{main_folder}+{sub_folder}+\")]\n",
    "                        rel_ng_ins_ids = random.sample(related_ng_keys, min(NUM, len(related_ng_keys)))\n",
    "\n",
    "                    # JSONã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ\n",
    "                    new_data[\"data\"][key_name] = {\n",
    "                        \"instruction\": \"\",\n",
    "                        \"answer\": \"\",\n",
    "                        \"flag\": \"query\",\n",
    "                        \"image_ids\": [key_name],\n",
    "                        \"rel_ok_ins_ids\": rel_ok_ins_ids,\n",
    "                        \"rel_ng_ins_ids\": rel_ng_ins_ids,\n",
    "                        \"label\": i\n",
    "                    }\n",
    "                i += 1\n",
    "\n",
    "    # æ—¢å­˜ã®JSONãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸\n",
    "    old_data[\"data\"].update(new_data[\"data\"])\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    # old_json_path = \"output.json\"\n",
    "    with open(old_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(old_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "query_path = \"./full_images_train_query\"\n",
    "NUM = 5 # è‰¯å“ã€ä¸è‰¯å“ã‚’ãã‚Œãã‚Œ5æšãšã¤é¸ã¶\n",
    "generate_and_merge_json_from_directory(query_path, output_json_path, NUM)\n",
    "\n",
    "output_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "query_path = \"./full_images_val_query\"\n",
    "NUM = 5 # è‰¯å“ã€ä¸è‰¯å“ã‚’ãã‚Œãã‚Œ5æšãšã¤é¸ã¶\n",
    "generate_and_merge_json_from_directory(query_path, output_json_path, NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "def count_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "        \n",
    "    entries = [item for item in data.values() if item[\"flag\"] == \"context\"]\n",
    "    count = len(entries)\n",
    "    print(f\"context: {count}\")\n",
    "        \n",
    "    entries = [item for item in data.values() if item[\"flag\"] == \"query\"]\n",
    "    count = len(entries)\n",
    "    print(f\"query: {count}\")\n",
    "\n",
    "print(\"train\")\n",
    "json_path = f\"{base_folder}/train_instructions.json\"\n",
    "count_data(json_path)\n",
    "\n",
    "print(\"val\")\n",
    "json_path = f\"{base_folder}/val_instructions.json\"\n",
    "count_data(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆå½¢å¼ã®æ–‡å­—åˆ—ã‚’ä½œæˆã™ã‚‹é–¢æ•°\n",
    "def generate_list_string(items):\n",
    "    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    \n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "\n",
    "def fill_instruction_and_answer(json_path, train_context_dir):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key, value in data.items():\n",
    "        # ã‚­ãƒ¼ã®åå‰ã‹ã‚‰è¦ªãƒ•ã‚©ãƒ«ãƒ€åã¨å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’èªè­˜ã™ã‚‹\n",
    "        parent_folder, child_folder, _ = key.split(\"+\")\n",
    "\n",
    "        # \"./1_train_context/è¦ªãƒ•ã‚©ãƒ«ãƒ€å\"ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ãã®ä¸­ã«ã‚ã‚‹å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹\n",
    "        subfolder_list = os.listdir(os.path.join(train_context_dir, parent_folder))\n",
    "        subfolder_list = [folder for folder in subfolder_list if folder != \"None\"] # æ¬ é™¥åã®ã¿ãƒªã‚¹ãƒˆåŒ–\n",
    "\n",
    "        # \"instruction\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        subfolder_string = generate_list_string(subfolder_list)\n",
    "        parent_folder__ = parent_folder.replace('_', ' ')\n",
    "        # long\n",
    "        value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects such as {subfolder_string}?'\n",
    "        # short\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects?'\n",
    "        # latest long\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this {parent_folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        # not object name\n",
    "        # value[\"instruction\"] = f'Does this image have any defects? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        # product\n",
    "        # value[\"instruction\"] = f'This is an image of {parent_folder__}. Does this image have any defects? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "        \n",
    "        \n",
    "        # \"answer\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        # if child_folder == \"None\": # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "        if child_folder == parent_folder: # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "            value[\"answer\"] = f'No. This {parent_folder__} does not have any defects such as {subfolder_string}, so it is non-defective.'\n",
    "            # value[\"answer\"] = f'No. This {parent_folder__} does not have any defects, so it is non-defective.'\n",
    "            # value[\"answer\"] = f'No.'\n",
    "            # value[\"answer\"] = f'No None'\n",
    "        else:\n",
    "            child_folder = child_folder.replace('_', ' ')\n",
    "            value[\"answer\"] = f'Yes. This {parent_folder__} has some {child_folder}, so it is defective.'\n",
    "            # value[\"answer\"] = f'Yes.'\n",
    "            # value[\"answer\"] = f'Yes {child_folder}'\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "train_context_dir = \"./full_images_train_context\" # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ok (ã‚¯ã‚¨ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã®ãŸã‚)\n",
    "fill_instruction_and_answer(output_json_path, train_context_dir)\n",
    "\n",
    "output_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "train_context_dir = \"./full_images_val_context\" # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ok (ã‚¯ã‚¨ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã®ãŸã‚)\n",
    "fill_instruction_and_answer(output_json_path, train_context_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_train.jsonã®ä½œæˆï¼ˆFrame3ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    train_data = {}\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key, value in data.items():\n",
    "        # 'flag'=='query'ã®ã‚­ãƒ¼ã‚’é¸æŠ\n",
    "        if value[\"flag\"] == \"query\":\n",
    "            # \"rel_ok_ins_ids\"ã¨\"rel_ng_ins_ids\"ã‚’èª­ã¿è¾¼ã¿\n",
    "            ok_ids = value[\"rel_ok_ins_ids\"]\n",
    "            ng_ids = value[\"rel_ng_ins_ids\"]\n",
    "\n",
    "            # ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜\n",
    "            combinations = []\n",
    "            for ok_id in ok_ids:\n",
    "                for ng_id in ng_ids:\n",
    "                    combinations.append([ok_id, ng_id])\n",
    "\n",
    "            # ãƒªã‚¹ãƒˆå†…ã®å„çµ„å†…ã®è¦ç´ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            for combination in combinations:\n",
    "                random.shuffle(combination)\n",
    "\n",
    "            # ã‚­ãƒ¼åã‚’æ›´æ–°ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\n",
    "            random.shuffle(combinations)\n",
    "            for i, combination in enumerate(combinations[:NUM_PAIRS]):\n",
    "                train_data[f\"{key}={i}\"] = combination\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(train_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "NUM_PAIRS = 25 # 1ã¤ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹çµ„ã¿åˆã‚ã›ã®æ•°\n",
    "input_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/train_pairs{NUM_PAIRS}_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS)\n",
    "\n",
    "NUM_PAIRS__ = 1 # 1ã¤ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹çµ„ã¿åˆã‚ã›ã®æ•°ï¼ˆvalã¯1ã«ã™ã‚‹ã¨ã‚¯ã‚¨ãƒªãŒä¸€å›ã®ã¿ï¼‰\n",
    "input_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/val_pairs{NUM_PAIRS__}_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path, NUM_PAIRS__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "def count_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    count = len(data.keys())\n",
    "    print(f\"ãƒšã‚¢æ•°: {count}\")\n",
    "    # oks = [key for key in data.keys() if key.split(\"+\")[1] == \"None\"] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "    oks = [key for key in data.keys() if key.split(\"+\")[0] == key.split(\"+\")[1]] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "    print(f\"ã‚¯ã‚¨ãƒªè‰¯å“: {len(oks)}\")\n",
    "    # ngs = [key for key in data.keys() if key.split(\"+\")[1] != \"None\"] # \"æ¬ é™¥\"ã®å ´åˆ\n",
    "    ngs = [key for key in data.keys() if key.split(\"+\")[0] != key.split(\"+\")[1]] # \"è£½å“+æ¬ é™¥\"ã®å ´åˆ\n",
    "    print(f\"ã‚¯ã‚¨ãƒªä¸è‰¯å“: {len(ngs)}\")\n",
    "    \n",
    "print(\"train\")\n",
    "# NUM_PAIRS = 25\n",
    "json_path = f\"{base_folder}/train_pairs{NUM_PAIRS}_train.json\"\n",
    "count_data(json_path)\n",
    "\n",
    "print(\"val\")\n",
    "# NUM_PAIRS__ = 1\n",
    "json_path = f\"{base_folder}/val_pairs{NUM_PAIRS__}_train.json\"\n",
    "count_data(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹.jsonã®ä½œæˆï¼ˆå…±é€šï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€€â†’ã€€ã‚¯ã‚¨ãƒªã®é †ã§å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def image_to_urlsafe_base64_png(img_path):\n",
    "    \"\"\"ç”»åƒã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§PNGã«å¤‰æ›ã—ã€ãã®å¾ŒURL-safeãªBase64ã«å¤‰æ›ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    with Image.open(img_path) as image:\n",
    "        # CMYKãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        if image.mode == 'CMYK':\n",
    "            image = image.convert('RGB')\n",
    "        # ãƒ‘ãƒ¬ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBAãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        if image.mode == 'P':\n",
    "            image = image.convert('RGBA')\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.urlsafe_b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return img_str\n",
    "\n",
    "def create_visual_inspection(input_json_path, base_folder, output_json_path):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # æ—¢å­˜ã®visual_inspection.jsonãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€\n",
    "    if os.path.exists(output_json_path):\n",
    "        with open(output_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            visual_data = json.load(f)\n",
    "    else:\n",
    "        visual_data = {}\n",
    "\n",
    "    extensions = ['.png', '.jpg', '.jpeg']\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key in data.keys():\n",
    "        parent_folder, child_folder, image_name = key.split(\"+\")\n",
    "        \n",
    "        # å„æ‹¡å¼µå­ã‚’è©¦ã—ã¦ã€å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        for ext in extensions:\n",
    "            img_path = os.path.join(base_folder, parent_folder, child_folder, image_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                # ç”»åƒã‚’URL-safeãªBase64 PNGå½¢å¼ã«å¤‰æ›\n",
    "                visual_data[key] = image_to_urlsafe_base64_png(img_path)\n",
    "                break\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(visual_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "\"\"\" train \"\"\"\n",
    "input_json_path = f\"{base_folder}/train_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/train_images.json\"\n",
    "context_folder = \"./full_images_train_context\"  # ç”»åƒèª­ã¿è¾¼ã¿ç”¨\n",
    "create_visual_inspection(input_json_path, context_folder, output_json_path)\n",
    "query_folder = \"./full_images_train_query\"  # ç”»åƒèª­ã¿è¾¼ã¿ç”¨\n",
    "create_visual_inspection(input_json_path, query_folder, output_json_path)\n",
    "\n",
    "\"\"\" val \"\"\"\n",
    "input_json_path = f\"{base_folder}/val_instructions.json\"\n",
    "output_json_path = f\"{base_folder}/val_images.json\"\n",
    "context_folder = \"./full_images_val_context\"  # ç”»åƒèª­ã¿è¾¼ã¿ç”¨\n",
    "create_visual_inspection(input_json_path, context_folder, output_json_path)\n",
    "query_folder = \"./full_images_val_query\"  # ç”»åƒèª­ã¿è¾¼ã¿ç”¨\n",
    "create_visual_inspection(input_json_path, query_folder, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# æ•°ã®ç¢ºèª\n",
    "def count_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    count = len(data.keys())\n",
    "    print(f\"ç”»åƒæšæ•°: {count}\")\n",
    "    \n",
    "print(\"train\")\n",
    "json_path = f\"{base_folder}/train_images.json\"\n",
    "count_data(json_path)\n",
    "\n",
    "print(\"val\")\n",
    "json_path = f\"{base_folder}/val_images.json\"\n",
    "count_data(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¬ é™¥åå½“ã¦ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ•ã‚©ãƒ«ãƒ€ã®é¸åˆ¥\n",
    "1. å­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒæšæ•°ãŒ0æšâ†’å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "2. å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ0â†’è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "\n",
    "â€» å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’ \"None\" or \"æ¬ é™¥\"ã«ã—ã¦ãŠã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹\n",
    "base_folder = \"./full_images_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    try:\n",
    "        shutil.rmtree(folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼š{e}\")\n",
    "\n",
    "def count_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    image_count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "if os.path.isdir(base_folder):  # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    for parent_folder in os.listdir(base_folder):\n",
    "        parent_folder_path = os.path.join(base_folder, parent_folder)\n",
    "        if os.path.isdir(parent_folder_path):  # parent_folder_pathãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "            for child_folder in os.listdir(parent_folder_path):\n",
    "                child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "                if os.path.isdir(child_folder_path):  # child_folder_pathãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "                    num_images = count_image_files(child_folder_path)\n",
    "                    if num_images == 0:\n",
    "                        print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "                        delete_folder(child_folder_path)\n",
    "            if len(os.listdir(parent_folder_path)) == 0:\n",
    "                print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {parent_folder_path}\")\n",
    "                delete_folder(parent_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ‹¡å¼µå­ã‚’é™¤ã„ãŸãƒ•ã‚©ãƒ«ãƒ€åã®é‡è¤‡ã‚’ä¿®æ­£<br>\n",
    "punctured_tireå†…ã«\"image_49.jpeg\"ã¨\"image_49.jpg\"ãŒå­˜åœ¨ã—ãŸãŸã‚ã€å…¨ã¦ãƒã‚§ãƒƒã‚¯ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèª\n",
    "if os.path.isdir(base_folder):\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "    for folder_name in os.listdir(base_folder):\n",
    "        subfolder_path = os.path.join(base_folder, folder_name)\n",
    "        \n",
    "        if os.path.isdir(subfolder_path):  # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "            # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—\n",
    "            for subfolder_name in os.listdir(subfolder_path):\n",
    "                child_folder_path = os.path.join(subfolder_path, subfolder_name)\n",
    "                \n",
    "                if os.path.isdir(child_folder_path):  # å­ãƒ•ã‚©ãƒ«ãƒ€ã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                    # å­ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "                    files = os.listdir(child_folder_path)\n",
    "                    \n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ•°å­—éƒ¨åˆ†ã‚’å–å¾—ã—ã€ãƒªã‚¹ãƒˆã«ä¿å­˜\n",
    "                    numbers = [int(re.search(r'\\d+', file).group()) for file in files if re.search(r'\\d+', file)]\n",
    "                    \n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›´\n",
    "                    for file in files:\n",
    "                        match = re.search(r'\\d+', file)\n",
    "                        if match:\n",
    "                            number = int(match.group())\n",
    "                            \n",
    "                            # åŒã˜ç•ªå·ãŒè¤‡æ•°å­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "                            if numbers.count(number) > 1:\n",
    "                                new_number = 0\n",
    "                                \n",
    "                                # é‡è¤‡ã—ã¦ã„ãªã„ç•ªå·ã‚’æ¢ã™\n",
    "                                while new_number in numbers:\n",
    "                                    new_number += 1\n",
    "                                \n",
    "                                # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ\n",
    "                                new_file = file.replace(str(number), str(new_number))\n",
    "                                new_file_path = os.path.join(child_folder_path, new_file)\n",
    "                                \n",
    "                                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´\n",
    "                                os.rename(\n",
    "                                    os.path.join(child_folder_path, file), \n",
    "                                    new_file_path\n",
    "                                )\n",
    "                                \n",
    "                                # ãƒ•ã‚¡ã‚¤ãƒ«åã®å¤‰æ›´ã‚’è¡¨ç¤º\n",
    "                                print(f'{child_folder_path}/{file} -> {new_file_path}')\n",
    "                                \n",
    "                                # ãƒªã‚¹ãƒˆã‚’æ›´æ–°\n",
    "                                numbers.remove(number)\n",
    "                                numbers.append(new_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordã«æ¬ é™¥åãŒéƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹\n",
    "# import os\n",
    "\n",
    "# def search_folders(base_folder, keyword):\n",
    "#     found_folders = []\n",
    "    \n",
    "#     for root, dirs, files in os.walk(base_folder):\n",
    "#         for directory in dirs:\n",
    "#             if keyword in directory:\n",
    "#                 found_folders.append(os.path.join(root, directory))\n",
    "    \n",
    "#     return found_folders\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     keyword = 'worm'  # æ¢ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚ã“ã‚Œã‚‚å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "#     found_folders = search_folders(base_folder, keyword)\n",
    "    \n",
    "#     for folder in found_folders:\n",
    "#         print(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainï¼šval = 8ï¼š2 ã«åˆ†ã‘ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "\n",
    "src_dir = base_folder\n",
    "train_dir = './AC_full_train/'\n",
    "val_dir = './AC_full_val/'\n",
    "\n",
    "# trainã¨valã®å‰²åˆ\n",
    "ratio = 0.8\n",
    "\n",
    "# trainã¨valã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# src_dirå†…ã®å„ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦\n",
    "for root, dirs, files in os.walk(src_dir):\n",
    "    # ç”»åƒã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    images = [f for f in files if os.path.isfile(os.path.join(root, f))]\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # trainã¨valã«åˆ†å‰²\n",
    "    train_images = images[:int(ratio * len(images))]\n",
    "    val_images = images[int(ratio * len(images)):]\n",
    "    \n",
    "    # trainã¨valã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    train_folder = os.path.join(train_dir, os.path.relpath(root, src_dir))\n",
    "    val_folder = os.path.join(val_dir, os.path.relpath(root, src_dir))\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    \n",
    "    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(root, img), os.path.join(train_folder, img))\n",
    "    for img in val_images:\n",
    "        shutil.copy(os.path.join(root, img), os.path.join(val_folder, img))\n",
    "\n",
    "print('Images copied to train and val folders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valä¸­ã®ç”»åƒæšæ•°ãŒ1ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’trainã¸ç§»å‹•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç§»å‹•ã•ã›ã‚‹é–¢æ•°\n",
    "def move_child_folder(src, dst):\n",
    "    \"\"\"\n",
    "    :param src: ç§»å‹•ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    :param dst: ç§»å‹•å…ˆã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    if os.path.exists(dst):\n",
    "        # ç§»å‹•å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ç§»å‹•\n",
    "        for file_name in os.listdir(src):\n",
    "            full_file_name = os.path.join(src, file_name)\n",
    "            if os.path.isfile(full_file_name):\n",
    "                shutil.move(full_file_name, dst)\n",
    "        # ã‚‚ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "        os.rmdir(src)\n",
    "    else:\n",
    "        # ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ç§»å‹•\n",
    "        if not os.path.exists(os.path.dirname(dst)):\n",
    "            os.makedirs(os.path.dirname(dst))\n",
    "        shutil.move(src, os.path.join(os.path.dirname(dst), os.path.basename(src)))\n",
    "\n",
    "# è¦ªãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ç§»å‹•ã•ã›ã‚‹é–¢æ•°\n",
    "def move_parent_folder(src, dst):\n",
    "    \"\"\"\n",
    "    :param src: ç§»å‹•ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    :param dst: ç§»å‹•å…ˆã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    for src_dir, dirs, files in os.walk(src):\n",
    "        dst_dir = src_dir.replace(src, dst, 1)\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        for file_ in files:\n",
    "            src_file = os.path.join(src_dir, file_)\n",
    "            dst_file = os.path.join(dst_dir, file_)\n",
    "            if os.path.exists(dst_file):\n",
    "                os.remove(dst_file)\n",
    "            shutil.move(src_file, dst_dir)\n",
    "    shutil.rmtree(src)\n",
    "    \n",
    "# ç§»å‹•å…ˆã®ãƒ‘ã‚¹ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "def change_path(path, new_base):\n",
    "    # ãƒ‘ã‚¹ã‚’åˆ†å‰²\n",
    "    parts = path.split('/')\n",
    "    # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ã§ãƒ‘ã‚¹ã‚’å†æ§‹ç¯‰\n",
    "    parts[1] = new_base\n",
    "    return '/'.join(parts)\n",
    "\n",
    "# ä¸ãˆã‚‰ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ•°ãˆã‚‹é–¢æ•°\n",
    "def count_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’è¿½åŠ \n",
    "    image_count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "def val(val_dir, val_base):\n",
    "    if os.path.isdir(val_dir):  # val_dirãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "        for parent_folder in os.listdir(val_dir):\n",
    "            parent_folder_path = os.path.join(val_dir, parent_folder)\n",
    "            if os.path.isdir(parent_folder_path):  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                for child_folder in os.listdir(parent_folder_path):\n",
    "                    child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "                    if os.path.isdir(child_folder_path):  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                        num_images = count_image_files(child_folder_path)\n",
    "                        if num_images == 1:\n",
    "                            new_path = change_path(child_folder_path, val_base)\n",
    "                            print(f\"ç§»å‹•ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, ç§»å‹•å…ˆ: {new_path}, æšæ•°: {num_images}\")\n",
    "                            move_child_folder(child_folder_path, new_path)\n",
    "\n",
    "val_dir = './AC_full_val/'\n",
    "val_base = 'AC_full_train'\n",
    "val(val_dir, val_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valã¨trainã‹ã‚‰ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "def delete_empty_folders(base_folder):\n",
    "    if os.path.isdir(base_folder):\n",
    "        for parent_folder in os.listdir(base_folder):\n",
    "            parent_folder_path = os.path.join(base_folder, parent_folder)\n",
    "            if os.path.isdir(parent_folder_path):  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                for child_folder in os.listdir(parent_folder_path):\n",
    "                    child_folder_path = os.path.join(parent_folder_path, child_folder)\n",
    "                    if os.path.isdir(child_folder_path):  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ç¢ºèª\n",
    "                        num_images = count_image_files(child_folder_path)\n",
    "                        # ç”»åƒæšæ•°ãŒ2æšæœªæº€ã®å ´åˆã€ãã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "                        if num_images==0:\n",
    "                            print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {child_folder_path}, æšæ•°: {num_images}\")\n",
    "                            delete_folder(child_folder_path)\n",
    "                # è¦ªãƒ•ã‚©ãƒ«ãƒ€å†…ã®å­ãƒ•ã‚©ãƒ«ãƒ€æ•°ãŒ0ã®å ´åˆã€è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
    "                if len(os.listdir(parent_folder_path))==0 :\n",
    "                    print(f\"å‰Šé™¤ãƒ•ã‚©ãƒ«ãƒ€å: {parent_folder_path}\")\n",
    "                    delete_folder(parent_folder_path)\n",
    "\n",
    "print(train_dir)\n",
    "delete_empty_folders(train_dir)\n",
    "print(val_dir)\n",
    "delete_empty_folders(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_missing_folders(folder1, folder2):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸ2ã¤ã®ãƒ•ã‚©ãƒ«ãƒ€é–“ã§å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã¦å‡ºåŠ›ã—ã¾ã™ã€‚\n",
    "    Args:\n",
    "        folder1 (str): æœ€åˆã®ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "        folder2 (str): 2ç•ªç›®ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€1å†…ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    folders1 = os.listdir(folder1)\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€2å†…ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    folders2 = os.listdir(folder2)\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ«ãƒ€1ã«å­˜åœ¨ã—ã€ãƒ•ã‚©ãƒ«ãƒ€2ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "    missing_folders = [folder for folder in folders1 if folder not in folders2]\n",
    "\n",
    "    # çµæœã‚’å‡ºåŠ›\n",
    "    print(f\"ãƒ•ã‚©ãƒ«ãƒ€ '{folder1}' ã«ã‚ã£ã¦ '{folder2}' ã«å­˜åœ¨ã—ãªã„ãƒ•ã‚©ãƒ«ãƒ€:\")\n",
    "    for folder in missing_folders:\n",
    "        print(folder)\n",
    "\n",
    "# ä½¿ç”¨ä¾‹\n",
    "folder1_path = 'AC_full_train'\n",
    "folder2_path = 'AC_full_val'\n",
    "find_missing_folders(folder1_path, folder2_path)\n",
    "find_missing_folders(folder2_path, folder1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_instructions.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•°å­—éƒ¨åˆ†ã‚’è€ƒæ…®ã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼é–¢æ•°\n",
    "    \"\"\"\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def generate_json_from_directory(directory_path, output_json_path):\n",
    "    output = {\"data\": {}}\n",
    "    i = 0\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "    for main_folder in os.listdir(directory_path):\n",
    "        main_folder_path = os.path.join(directory_path, main_folder)\n",
    "        \n",
    "        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã‚ã‚‹ã‹ã®ç¢ºèª\n",
    "        if os.path.isdir(main_folder_path):\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»\n",
    "            for sub_folder in os.listdir(main_folder_path):\n",
    "                sub_folder_path = os.path.join(main_folder_path, sub_folder)\n",
    "                \n",
    "                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜‡é †ã«èµ°æŸ»\n",
    "                for image_file in sorted(os.listdir(sub_folder_path), key=natural_sort_key):\n",
    "                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’é™¤å»\n",
    "                    image_name_without_extension = os.path.splitext(image_file)[0]\n",
    "                    \n",
    "                    # ã‚­ãƒ¼ã®åå‰ã‚’ç”Ÿæˆ\n",
    "                    key_name = f\"{main_folder}+{sub_folder}+{image_name_without_extension}\"\n",
    "                    \n",
    "                    # JSONã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”Ÿæˆ\n",
    "                    output[\"data\"][key_name] = {\n",
    "                        \"instruction\": \"\",\n",
    "                        \"answer\": \"\",\n",
    "                        \"image_ids\": [key_name],\n",
    "                        \"label\": i\n",
    "                    }\n",
    "                i += 1\n",
    "    \n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(output, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "base_path = \"./AC_full\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "context_path = \"./AC_full_train\"\n",
    "output_json_path = f\"{base_path}/AC_train_instructions.json\"\n",
    "generate_json_from_directory(context_path, output_json_path)\n",
    "\n",
    "# base_path = \"./AC_jsons\"\n",
    "# os.makedirs(base_path, exist_ok=True)\n",
    "context_path = \"./AC_full_val\"\n",
    "output_json_path = f\"{base_path}/AC_val_instructions.json\"\n",
    "generate_json_from_directory(context_path, output_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆå½¢å¼ã®æ–‡å­—åˆ—ã‚’ä½œæˆã™ã‚‹é–¢æ•°\n",
    "def generate_list_string(items):\n",
    "    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    \n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "\n",
    "def fill_instruction_and_answer(json_path, train_context_dir):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key, value in data.items():\n",
    "        # ã‚­ãƒ¼ã®åå‰ã‹ã‚‰è¦ªãƒ•ã‚©ãƒ«ãƒ€åã¨å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’èªè­˜ã™ã‚‹\n",
    "        parent_folder, child_folder, _ = key.split(\"+\")\n",
    "\n",
    "        # \"./1_train_context/è¦ªãƒ•ã‚©ãƒ«ãƒ€å\"ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ãã®ä¸­ã«ã‚ã‚‹å­ãƒ•ã‚©ãƒ«ãƒ€åã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹\n",
    "        subfolder_list = os.listdir(os.path.join(train_context_dir, parent_folder))\n",
    "        subfolder_list = [folder for folder in subfolder_list if folder != parent_folder] # è¦ªãƒ•ã‚©ãƒ«ãƒ€åã‚’é™¤å¤–\n",
    "\n",
    "        # \"instruction\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        subfolder_string = generate_list_string(subfolder_list)\n",
    "        parent_folder__ = parent_folder.replace('_', ' ')\n",
    "        # value[\"instruction\"] = f'What are the defects present in this image? If there are none, please say None.'\n",
    "        value[\"instruction\"] = f'Does this image have any defects? If there are any defects, please provide the defect name. If not, please say None.'\n",
    "       \n",
    "        # \"answer\"ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "        if child_folder==\"None\":\n",
    "            # value[\"answer\"] = f'None'\n",
    "            value[\"answer\"] = f'No None'\n",
    "        else:\n",
    "            child_folder = child_folder.replace('_', ' ')\n",
    "            # value[\"answer\"] = f'{child_folder}'\n",
    "            value[\"answer\"] = f'Yes {child_folder}'\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "json_path = f\"{base_path}/AC_train_instructions.json\"\n",
    "dir = \"./AC_full_train\"\n",
    "fill_instruction_and_answer(json_path, dir)\n",
    "\n",
    "json_path = f\"{base_path}/AC_val_instructions.json\"\n",
    "dir = \"./AC_full_val\"\n",
    "fill_instruction_and_answer(json_path, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "\"\"\"  train \"\"\"\n",
    "print(\"train\")\n",
    "json_path = f\"{base_path}/AC_train_instructions.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "    \n",
    "entries = [item for item in data.values() if item[\"answer\"] == \"None\"]\n",
    "count = len(entries)\n",
    "print(f\"è‰¯å“: {count}\")\n",
    "    \n",
    "entries = [item for item in data.values() if item[\"answer\"] != \"None\"]\n",
    "count = len(entries)\n",
    "print(f\"ä¸è‰¯å“: {count}\")\n",
    "\n",
    "\"\"\"  val \"\"\"\n",
    "print(\"val\")\n",
    "json_path = f\"{base_path}/AC_val_instructions.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "    \n",
    "entries = [item for item in data.values() if item[\"answer\"] == \"None\"]\n",
    "count = len(entries)\n",
    "print(f\"è‰¯å“: {count}\")\n",
    "    \n",
    "entries = [item for item in data.values() if item[\"answer\"] != \"None\"]\n",
    "count = len(entries)\n",
    "print(f\"ä¸è‰¯å“: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_train.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def create_visual_inspection_train(input_json_path, output_json_path):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    train_data = {}\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    keys = []\n",
    "    for key, value in data.items():\n",
    "        keys.append(key)\n",
    "    \n",
    "    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\n",
    "    random.shuffle(keys)\n",
    "    for key_name in keys:\n",
    "        train_data[f\"{key_name}\"] = []\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(train_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "input_json_path = f\"{base_path}/AC_train_instructions.json\"\n",
    "output_json_path = f\"{base_path}/AC_train_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path)\n",
    "\n",
    "input_json_path = f\"{base_path}/AC_val_instructions.json\"\n",
    "output_json_path = f\"{base_path}/AC_val_train.json\"\n",
    "create_visual_inspection_train(input_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°ã®ç¢ºèª\n",
    "json_path = f\"{base_path}/AC_train_train.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "count = len(data.keys())\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {count}\")\n",
    "\n",
    "json_path = f\"{base_path}/AC_val_train.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "count = len(data.keys())\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€€â†’ã€€ã‚¯ã‚¨ãƒªã®é †ã§å®Ÿè¡Œã™ã‚‹ã“ã¨\n",
    "\n",
    "def image_to_urlsafe_base64_png(img_path):\n",
    "    \"\"\"ç”»åƒã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§PNGã«å¤‰æ›ã—ã€ãã®å¾ŒURL-safeãªBase64ã«å¤‰æ›ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    with Image.open(img_path) as image:\n",
    "        # CMYKãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        if image.mode == 'CMYK':\n",
    "            image = image.convert('RGB')\n",
    "        # ãƒ‘ãƒ¬ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBAãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        if image.mode == 'P':\n",
    "            image = image.convert('RGBA')\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.urlsafe_b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return img_str\n",
    "\n",
    "def create_visual_inspection(input_json_path, base_folder, output_json_path):\n",
    "    # JSONã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # æ—¢å­˜ã®visual_inspection.jsonãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€\n",
    "    if os.path.exists(output_json_path):\n",
    "        with open(output_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            visual_data = json.load(f)\n",
    "    else:\n",
    "        visual_data = {}\n",
    "\n",
    "    extensions = ['.png', '.jpg', '.jpeg']\n",
    "\n",
    "    # ['data']ã‹ã‚‰ã‚­ãƒ¼ã‚’é †ã«èª­ã¿è¾¼ã‚€\n",
    "    for key in data.keys():\n",
    "        parent_folder, child_folder, image_name = key.split(\"+\")\n",
    "        \n",
    "        # å„æ‹¡å¼µå­ã‚’è©¦ã—ã¦ã€å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        for ext in extensions:\n",
    "            img_path = os.path.join(base_folder, parent_folder, child_folder, image_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                # ç”»åƒã‚’URL-safeãªBase64 PNGå½¢å¼ã«å¤‰æ›\n",
    "                visual_data[key] = image_to_urlsafe_base64_png(img_path)\n",
    "                break\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(visual_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "base_folder = \"./AC_full_train\"  # ã“ã‚Œã¯ä¸€ã¤ä¸Šã®ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "input_json_path = f\"{base_path}/AC_train_instructions.json\"\n",
    "output_json_path = f\"{base_path}/AC_train.json\"\n",
    "create_visual_inspection(input_json_path, base_folder, output_json_path)\n",
    "\n",
    "base_folder = \"./AC_full_val\"  # ã“ã‚Œã¯ä¸€ã¤ä¸Šã®ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "input_json_path = f\"{base_path}/AC_val_instructions.json\"\n",
    "output_json_path = f\"{base_path}/AC_val.json\"\n",
    "create_visual_inspection(input_json_path, base_folder, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# æ•°ã®ç¢ºèª\n",
    "json_path = f\"{base_path}/AC_train.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "count = len(data.keys())\n",
    "print(f\"trainç”»åƒæšæ•°: {count}\")\n",
    "\n",
    "json_path = f\"{base_path}/AC_val.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "count = len(data.keys())\n",
    "print(f\"valç”»åƒæšæ•°: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å·®åˆ†ã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ<br>\n",
    "1. è£½å“ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ --> product_no / product_yes\n",
    "2. åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ --> existence_same_no / existence_same_yes\n",
    "3. åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ --> mode_same_no / mode_same_yes\n",
    "4. ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ --> existence_different_no / existence_different_yes\n",
    "5. ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ --> mode_different_no / mode_different_yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_image_folder = \"./AC_full_train/\" # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹\n",
    "output_json_path = \"./jsons/VI_SD/AC_full_train/\" # jsonã®ä¿å­˜ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹\n",
    "os.makedirs(output_json_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®å¯è¦–åŒ–\n",
    "def list_folders(path, indent=0):\n",
    "    # æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹å†…ã®ã™ã¹ã¦ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    entries = os.listdir(path)\n",
    "\n",
    "    for entry in entries:\n",
    "        full_path = os.path.join(path, entry)\n",
    "\n",
    "        # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆ\n",
    "        if os.path.isdir(full_path):\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã—ã¦è¡¨ç¤º\n",
    "            print(\"  \" * indent + f\"ğŸ“ {entry}\")\n",
    "            # å†å¸°çš„ã«ãƒ•ã‚©ãƒ«ãƒ€å†…ã‚’æ¢ç´¢\n",
    "            list_folders(full_path, indent + 1)\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’è¡¨ç¤º\n",
    "\n",
    "print(f\"ğŸ“ {base_image_folder}\")\n",
    "list_folders(base_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def image_to_urlsafe_base64_png(img_path):\n",
    "    \"\"\"ç”»åƒã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§PNGã«å¤‰æ›ã—ã€ãã®å¾ŒURL-safeãªBase64ã«å¤‰æ›ã™ã‚‹é–¢æ•°\"\"\"\n",
    "    with Image.open(img_path) as image:\n",
    "        # CMYKãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        if image.mode == 'CMYK':\n",
    "            image = image.convert('RGB')\n",
    "        # ãƒ‘ãƒ¬ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBAãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›\n",
    "        elif image.mode == 'P':\n",
    "            image = image.convert('RGBA')\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.urlsafe_b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return img_str\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg'])\n",
    "\n",
    "def create_images_json(base_folder, output_json_path):\n",
    "    visual_data = {}\n",
    "\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èµ°æŸ»\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            if is_image_file(file):\n",
    "                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ§‹æˆè¦ç´ ã‚’å–å¾—\n",
    "                path_parts = os.path.normpath(root).split(os.sep)\n",
    "                if len(path_parts) > 1:\n",
    "                    parent_folder = path_parts[-2]\n",
    "                    child_folder = path_parts[-1]\n",
    "                else:\n",
    "                    continue  # ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "\n",
    "                # ã‚­ãƒ¼åã®ç”Ÿæˆ\n",
    "                clear_output(wait=True)\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                key = f\"{parent_folder}+{child_folder}+{file_name}\"\n",
    "                print(key)\n",
    "                \n",
    "                # ç”»åƒã‚’URL-safeãªBase64 PNGå½¢å¼ã«å¤‰æ›\n",
    "                img_path = os.path.join(root, file)\n",
    "                print(img_path)\n",
    "                visual_data[key] = image_to_urlsafe_base64_png(img_path)\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(visual_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "output_json_name = \"train_images.json\"\n",
    "create_images_json(base_image_folder, os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# ç¢ºèª\n",
    "images = {}\n",
    "images_path=os.path.join(output_json_path, output_json_name)\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—ã‹ã‚‰ç”»åƒå¯è¦–åŒ–\n",
    "keys = list(images.keys())\n",
    "N = 200\n",
    "print(f\"total images: {len(keys)}\")\n",
    "print(keys[N])\n",
    "# base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿\n",
    "str_data1 = images[keys[N]]\n",
    "\n",
    "# ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "decoded_data1 = base64.urlsafe_b64decode(str_data1)\n",
    "\n",
    "# ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ãŸãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›\n",
    "image1 = Image.open(BytesIO(decoded_data1))\n",
    "\n",
    "# 1x1ã®subplotã‚’ä½œæˆã—ã¦ã€1æšã®ç”»åƒã‚’è¡¨ç¤º\n",
    "fig, axarr = plt.subplots(1, 1)\n",
    "\n",
    "axarr.imshow(image1)\n",
    "axarr.axis('off')  # è»¸ã‚’éè¡¨ç¤ºã«\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â—‹â—‹_instructions.jsonã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è£½å“ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (è£½å“ã«é•ã„ãŒãªã„å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®š\n",
    "random.seed(42)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg'])\n",
    "\n",
    "def has_two_or_more_images(child_folders):\n",
    "    # åˆè¨ˆç”»åƒæšæ•°ãŒ2æšä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã®ç¢ºèª\n",
    "    total_images = 0\n",
    "    for child_folder in child_folders:\n",
    "        total_images += len([file for file in os.listdir(child_folder) if is_image_file(file)])\n",
    "        if total_images >= 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    data = {}\n",
    "    instruction_id = 0\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é †ã«èµ°æŸ»\n",
    "    parent_folders = sorted([f.path for f in os.scandir(base_folder) if f.is_dir()])\n",
    "    for parent_folder in parent_folders:\n",
    "        pairs = []\n",
    "        child_folders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "        if has_two_or_more_images(child_folders):\n",
    "            # æœ€å¤§NUM_PAIRSå›ã®ãƒšã‚¢å½¢æˆã‚’è©¦ã¿ã‚‹\n",
    "            for _ in range(NUM_PAIRS):\n",
    "                pair = []\n",
    "                for _ in range(2):  # 2æšã®ç”»åƒã‚’é¸æŠ\n",
    "                    selected_image = None\n",
    "                    image_key = None\n",
    "                    # ãƒ©ãƒ³ãƒ€ãƒ ã«å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ\n",
    "                    selected_child_folder = random.choice(child_folders)\n",
    "                    images = [img for img in os.listdir(selected_child_folder) if is_image_file(img)]\n",
    "\n",
    "                    # ãƒ©ãƒ³ãƒ€ãƒ ã«ç”»åƒã‚’1æšé¸æŠ\n",
    "                    if images:\n",
    "                        selected_image = random.choice(images)\n",
    "                        image_key = f\"{os.path.basename(parent_folder)}+{os.path.basename(selected_child_folder)}+{os.path.splitext(selected_image)[0]}\"\n",
    "                        pair.append(image_key)\n",
    "                    else:\n",
    "                        print(f\"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {selected_child_folder}\")\n",
    "\n",
    "                # 2æšã®ç”»åƒãŒç•°ãªã‚‹å ´åˆã«ã®ã¿ãƒšã‚¢ã‚’è¿½åŠ \n",
    "                if len(pair) == 2 and pair[0] != pair[1]:\n",
    "                    pairs.append(pair)\n",
    "\n",
    "            # é‡è¤‡ã‚’é™¤å»\n",
    "            unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "\n",
    "            # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "            for pair in unique_pairs:\n",
    "                data[f\"SD_INS_{instruction_id}\"] = {\n",
    "                    \"instruction\": \"\", \n",
    "                    \"answer\": \"\", \n",
    "                    \"image_ids\": pair, \n",
    "                    \"flag\": \"product_no\"\n",
    "                }\n",
    "                instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "NUM_PAIRS = 20  # 1ã¤ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "output_json_name = \"train_instructions.json\"\n",
    "create_instructions_json(base_image_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"Are there any differences between the products in these images?\",\n",
    "    \"Do these images display distinct products?\",\n",
    "    \"Is there a discrepancy between the items shown in these photographs?\",\n",
    "    \"Are the products depicted here not identical?\",\n",
    "    \"Can you identify any dissimilarities in these products?\",\n",
    "    \"Are these two products not exactly the same?\",\n",
    "    \"Is there any variance in the products shown in these pictures?\",\n",
    "    \"Do the items in these images differ in any way?\",\n",
    "    \"Are the products presented in these images not the same?\",\n",
    "    \"Is there a divergence in the products seen in these pictures?\",\n",
    "    \"Can any distinctions be observed between these two products?\",\n",
    "    \"Are these two items dissimilar in any aspect?\",\n",
    "    \"Do these images represent two different products?\",\n",
    "    \"Are the products in these pictures not a perfect match?\",\n",
    "    \"Is there any non-identical feature in the products shown here?\", # 15\n",
    "    \"Are these two images of the same product?\",\n",
    "    \"Is there no difference between the products in these images?\",\n",
    "    \"Are the items shown in these photographs identical?\",\n",
    "    \"Do these pictures depict the same product?\",\n",
    "    \"Are these two products exactly alike?\",\n",
    "    \"Is there no variation between the products in these images?\",\n",
    "    \"Do the items in these images match perfectly?\",\n",
    "    \"Are the products presented here the same?\",\n",
    "    \"Is there no discrepancy in the products shown in these pictures?\",\n",
    "    \"Do these two images show an identical product?\",\n",
    "    \"Are the two products depicted here the same in every aspect?\",\n",
    "    \"Do these pictures display the same item?\",\n",
    "    \"Is there no distinct difference between the products in these images?\",\n",
    "    \"Are these images representing the same product?\",\n",
    "    \"Do these two products share complete similarity?\",\n",
    "]\n",
    "\n",
    "instructions_ja = [\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯ç•°ãªã‚‹è£½å“ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã«ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æã‹ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒä¸€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®è£½å“ã«ä½•ã‹ç›¸é•ç‚¹ãŒè¦‹ã¤ã‘ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã¯å®Œå…¨ã«åŒã˜ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«ä½•ã‹å¤‰åŒ–ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã«ä½•ã‹é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«è¦‹ã‚‰ã‚Œã‚‹è£½å“ã«ä½•ã‹ç›¸é•ç‚¹ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã®é–“ã«åŒºåˆ¥ãŒè¦³å¯Ÿã§ãã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ä½•ã‹ç‚¹ã§ç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯2ã¤ã®ç•°ãªã‚‹è£½å“ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã®è£½å“ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã§ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«åŒä¸€ã§ã¯ãªã„ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\", # 15\n",
    "    \"ã“ã‚Œã‚‰2ã¤ã®ç”»åƒã¯åŒã˜è£½å“ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«é•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯åŒä¸€ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜è£½å“ã‚’æã„ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2ã¤ã®è£½å“ã¯å…¨ãåŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«å¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®ç”»åƒã¯åŒä¸€ã®è£½å“ã‚’ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æã‹ã‚Œã¦ã„ã‚‹2ã¤ã®è£½å“ã¯ã™ã¹ã¦ã®é¢ã§åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«æ˜ç¢ºãªé•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯åŒã˜è£½å“ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã¯å®Œå…¨ã«é¡ä¼¼ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"No. Both images show the same product, {product_name}.\",\n",
    "    \"No. Both images display the same product, {product_name}.\",\n",
    "    \"No. The items in these photographs are identical and are both {product_name}.\",\n",
    "    \"No. These products are identical, and both are {product_name}.\",\n",
    "    \"No. There are no dissimilarities; both are {product_name}.\",\n",
    "    \"No. These two products are exactly the same, both being {product_name}.\",\n",
    "    \"No. There is no variance; both products are {product_name}.\",\n",
    "    \"No. The items in these images are the same, both {product_name}.\",\n",
    "    \"No. The products presented here are the same, both {product_name}.\",\n",
    "    \"No. There is no divergence; both are the same product, {product_name}.\",\n",
    "    \"No. No distinctions can be observed; both are {product_name}.\",\n",
    "    \"No. These two items are similar in every aspect, both {product_name}.\",\n",
    "    \"No. These images represent the same product, {product_name}.\",\n",
    "    \"No. The products in these pictures match perfectly, both are {product_name}.\",\n",
    "    \"No. There are no non-identical features; both are {product_name}.\",\n",
    "    \"Yes. Both of these images are of the same product, {product_name}.\",\n",
    "    \"Yes. There is no difference; both images show {product_name}.\",\n",
    "    \"Yes. The items shown in these photographs are identical, both {product_name}.\",\n",
    "    \"Yes. These pictures depict the same product, {product_name}.\",\n",
    "    \"Yes. These two products are exactly alike, both {product_name}.\",\n",
    "    \"Yes. There is no variation; both products are {product_name}.\",\n",
    "    \"Yes. The items in these images match perfectly, both are {product_name}.\",\n",
    "    \"Yes. The products presented here are the same, both {product_name}.\",\n",
    "    \"Yes. There is no discrepancy; both products shown are {product_name}.\",\n",
    "    \"Yes. These two images show an identical product, {product_name}.\",\n",
    "    \"Yes. The two products depicted are the same in every aspect, both {product_name}.\",\n",
    "    \"Yes. These pictures display the same item, {product_name}.\",\n",
    "    \"Yes. There is no distinct difference; both products are {product_name}.\",\n",
    "    \"Yes. These images represent the same product, {product_name}.\",\n",
    "    \"Yes. These two products share complete similarity, both are {product_name}.\",\n",
    "]\n",
    "\n",
    "answers_ja = [\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯ã©ã¡ã‚‰ã‚‚åŒã˜è£½å“ã€{product_name}ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯ã©ã¡ã‚‰ã‚‚åŒã˜è£½å“ã€{product_name}ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯åŒä¸€ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®è£½å“ã¯åŒä¸€ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰2ã¤ã®è£½å“ã¯å®Œå…¨ã«åŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã®è£½å“ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¯åŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã“ã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã‚‚åŒã˜è£½å“ã€{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚åŒºåˆ¥ã¯è¦³å¯Ÿã•ã‚Œã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯å…¨ã¦ã®é¢ã§åŒæ§˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯åŒã˜è£½å“ã€{product_name}ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ã“ã‚Œã‚‰ã®å†™çœŸã®è£½å“ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ãŠã‚Šã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚åŒä¸€ã§ã¯ãªã„ç‰¹å¾´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰2ã¤ã®ç”»åƒã¯ã©ã¡ã‚‰ã‚‚åŒã˜è£½å“ã€{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚é•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯ã©ã¡ã‚‰ã‚‚{product_name}ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯åŒä¸€ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜è£½å“ã€{product_name}ã‚’æã„ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰2ã¤ã®è£½å“ã¯å…¨ãåŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã®è£½å“ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ãŠã‚Šã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã“ã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®2ã¤ã®ç”»åƒã¯åŒä¸€ã®è£½å“ã€{product_name}ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã“ã«æã‹ã‚Œã¦ã„ã‚‹2ã¤ã®è£½å“ã¯ã™ã¹ã¦ã®é¢ã§åŒã˜ã§ã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜ã‚¢ã‚¤ãƒ†ãƒ ã€{product_name}ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æ˜ç¢ºãªé•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã®è£½å“ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰ã®ç”»åƒã¯åŒã˜è£½å“ã€{product_name}ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ã“ã‚Œã‚‰2ã¤ã®è£½å“ã¯å®Œå…¨ã«é¡ä¼¼ã—ã¦ãŠã‚Šã€ã©ã¡ã‚‰ã‚‚{product_name}ã§ã™ã€‚\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def fill_instructions_and_answers(output_json_path, instructions, answers):\n",
    "    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(output_json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "\n",
    "    # \"flag\"ãŒ\"product_no\"ã®é …ç›®ã ã‘ã‚’å‡¦ç†ã™ã‚‹\n",
    "    for key, value in data.items():\n",
    "        if value[\"flag\"] == \"product_no\":\n",
    "            # \"image_ids\"ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€åãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª\n",
    "            parent_folders = [image_id.split('+')[0].replace(\"_\", \" \") for image_id in value[\"image_ids\"]]\n",
    "            if parent_folders[0] == parent_folders[1]:\n",
    "                # instructions ã¨ answers ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "                question_index = random.randint(0, len(instructions) - 1)\n",
    "                value[\"instruction\"] = instructions[question_index]\n",
    "                value[\"answer\"] = answers[question_index].format(product_name=parent_folders[0])\n",
    "            else:\n",
    "                print(f\"Error: Image IDs in {key} do not have matching parent folder names.\")\n",
    "                break\n",
    "\n",
    "    # æ›´æ–°ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ä½¿ç”¨ä¾‹\n",
    "fill_instructions_and_answers(os.path.join(output_json_path, output_json_name), instructions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è£½å“ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (è£½å“ã«é•ã„ãŒã‚ã‚‹å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®š\n",
    "random.seed(42)\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg'])\n",
    "\n",
    "def load_existing_json(json_path):\n",
    "    \"\"\"æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ¬¡ã®instruction_idã¨ç›®æ¨™ã‚«ã‚¦ãƒ³ãƒˆã‚’æ±ºå®šã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        last_instruction_id = max([int(key.split(\"_\")[-1]) for key in data.keys()], default=0)\n",
    "        target_count = sum(1 for item in data.values() if item.get(\"flag\") == \"product_no\")\n",
    "        return data, last_instruction_id+1, target_count\n",
    "\n",
    "def create_instructions_json(base_folder, json_path):\n",
    "    # æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "    existing_data, next_instruction_id, target_count = load_existing_json(json_path)\n",
    "    parent_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    for _ in range(target_count):\n",
    "        # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«2ã¤é¸æŠ\n",
    "        selected_parents = random.sample(parent_folders, 2)\n",
    "        pair = []\n",
    "\n",
    "        for parent_folder in selected_parents:\n",
    "            child_folders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "            if child_folders:\n",
    "                # å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ\n",
    "                selected_child_folder = random.choice(child_folders)\n",
    "                images = [img for img in os.listdir(selected_child_folder) if is_image_file(img)]\n",
    "\n",
    "                # ãƒ©ãƒ³ãƒ€ãƒ ã«ç”»åƒã‚’1æšé¸æŠ\n",
    "                if images:\n",
    "                    selected_image = random.choice(images)\n",
    "                    image_key = f\"{os.path.basename(parent_folder)}+{os.path.basename(selected_child_folder)}+{os.path.splitext(selected_image)[0]}\"\n",
    "                    pair.append(image_key)\n",
    "                else:\n",
    "                    print(f\"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {selected_child_folder}\")\n",
    "\n",
    "        # 2æšã®ç”»åƒãŒé¸ã°ã‚ŒãŸå ´åˆã€JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "        if len(pair) == 2 and pair[0] != pair[1]:\n",
    "            existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "                \"instruction\": \"\",\n",
    "                \"answer\": \"\",\n",
    "                \"image_ids\": pair,\n",
    "                \"flag\": \"product_yes\"\n",
    "            }\n",
    "            next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "create_instructions_json(base_image_folder, os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_ja = [\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯ç•°ãªã‚‹è£½å“ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã«ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æã‹ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒä¸€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®è£½å“ã«ä½•ã‹ç›¸é•ç‚¹ãŒè¦‹ã¤ã‘ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã¯å®Œå…¨ã«åŒã˜ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«ä½•ã‹å¤‰åŒ–ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã«ä½•ã‹é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«è¦‹ã‚‰ã‚Œã‚‹è£½å“ã«ä½•ã‹ç›¸é•ç‚¹ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã®é–“ã«åŒºåˆ¥ãŒè¦³å¯Ÿã§ãã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ä½•ã‹ç‚¹ã§ç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯2ã¤ã®ç•°ãªã‚‹è£½å“ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã®è£½å“ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã§ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«åŒä¸€ã§ã¯ãªã„ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\", # 15\n",
    "    \"ã“ã‚Œã‚‰2ã¤ã®ç”»åƒã¯åŒã˜è£½å“ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«é•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«æ˜ ã£ã¦ã„ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã¯åŒä¸€ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜è£½å“ã‚’æã„ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2ã¤ã®è£½å“ã¯å…¨ãåŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«å¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¯å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã«ç¤ºã•ã‚Œã¦ã„ã‚‹è£½å“ã«ç›¸é•ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®ç”»åƒã¯åŒä¸€ã®è£½å“ã‚’ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã“ã«æã‹ã‚Œã¦ã„ã‚‹2ã¤ã®è£½å“ã¯ã™ã¹ã¦ã®é¢ã§åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®å†™çœŸã¯åŒã˜ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã®è£½å“ã«æ˜ç¢ºãªé•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®ç”»åƒã¯åŒã˜è£½å“ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®2ã¤ã®è£½å“ã¯å®Œå…¨ã«é¡ä¼¼ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"Yes. The first image shows {product_name1}, while the second image displays {product_name2}.\",\n",
    "    \"Yes. Image A is {product_name1}, and Image B is {product_name2}.\",\n",
    "    \"Yes. In the left image, we have {product_name1}, and in the right image, it's {product_name2}.\",\n",
    "    \"Yes. The product in the first picture is {product_name1}, whereas the second picture shows {product_name2}.\",\n",
    "    \"Yes. The item in the first image is {product_name1}, and in the second image, it's {product_name2}.\",\n",
    "    \"Yes. On the left side, the product is {product_name1}, and on the right, it's {product_name2}.\",\n",
    "    \"Yes. The first picture presents {product_name1}, while the second presents {product_name2}.\",\n",
    "    \"Yes. In image A, you see {product_name1}, and in image B, there's {product_name2}.\",\n",
    "    \"Yes. The left picture shows {product_name1}, and the right picture shows {product_name2}.\",\n",
    "    \"Yes. Picture A contains {product_name1}, and Picture B contains {product_name2}.\",\n",
    "    \"Yes. The first one is {product_name1}, and the second is {product_name2}.\",\n",
    "    \"Yes. Image on the left is {product_name1}, and on the right is {product_name2}.\",\n",
    "    \"Yes. First, we have {product_name1}, and second, we have {product_name2}.\",\n",
    "    \"Yes. The left one is {product_name1}, and the right one is {product_name2}.\",\n",
    "    \"Yes. Initially, {product_name1} is shown, followed by {product_name2}.\", # 15\n",
    "    \"No. The first image is {product_name1}, while the second image is {product_name2}.\",\n",
    "    \"No. Image A shows {product_name1}, and Image B shows {product_name2}.\",\n",
    "    \"No. The left image contains {product_name1}, and the right image contains {product_name2}.\",\n",
    "    \"No. In the first picture, we have {product_name1}, and in the second picture, {product_name2}.\",\n",
    "    \"No. The first image's product is {product_name1}, and the second's is {product_name2}.\",\n",
    "    \"No. On the left is {product_name1}, and on the right is {product_name2}.\",\n",
    "    \"No. The first picture depicts {product_name1}, and the second depicts {product_name2}.\",\n",
    "    \"No. In image A, it's {product_name1}, and in image B, it's {product_name2}.\",\n",
    "    \"No. The left picture has {product_name1}, and the right picture has {product_name2}.\",\n",
    "    \"No. Picture A represents {product_name1}, and Picture B represents {product_name2}.\",\n",
    "    \"No. The first product is {product_name1}, and the second is {product_name2}.\",\n",
    "    \"No. The left side shows {product_name1}, and the right side shows {product_name2}.\",\n",
    "    \"No. The first is {product_name1}, and the second is {product_name2}.\",\n",
    "    \"No. The left product is {product_name1}, and the right product is {product_name2}.\",\n",
    "    \"No. Initially, we see {product_name1}, followed by {product_name2}.\",\n",
    "]\n",
    "\n",
    "answers_ja = [\n",
    "    \"ã¯ã„ã€‚ä¸€æšç›®ã®ç”»åƒã¯{product_name1}ã§ã€äºŒæšç›®ã®ç”»åƒã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ç”»åƒAã¯{product_name1}ã€ç”»åƒBã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å·¦ã®ç”»åƒã«ã¯{product_name1}ãŒã‚ã‚Šã€å³ã®ç”»åƒã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã®å†™çœŸã«ã¯{product_name1}ãŒã€äºŒç•ªç›®ã®å†™çœŸã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã®ç”»åƒã«ã¯{product_name1}ãŒã€äºŒç•ªç›®ã®ç”»åƒã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å·¦å´ã«ã¯{product_name1}ãŒã€å³å´ã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã®å†™çœŸã¯{product_name1}ã‚’ã€äºŒç•ªç›®ã®å†™çœŸã¯{product_name2}ã‚’æç¤ºã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚ç”»åƒAã«ã¯{product_name1}ãŒã€ç”»åƒBã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å·¦ã®å†™çœŸã«ã¯{product_name1}ãŒã€å³ã®å†™çœŸã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å†™çœŸAã«ã¯{product_name1}ãŒã€å†™çœŸBã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã®è£½å“ã¯{product_name1}ã§ã€äºŒç•ªç›®ã®è£½å“ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å·¦å´ã®ç”»åƒã¯{product_name1}ã§ã€å³å´ã®ç”»åƒã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã«{product_name1}ãŒã€ãã®å¾Œã«{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚å·¦å´ã®è£½å“ã¯{product_name1}ã§ã€å³å´ã®è£½å“ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã¯ã„ã€‚æœ€åˆã«{product_name1}ãŒç¤ºã•ã‚Œã€ãã®å¾Œã«{product_name2}ãŒç¤ºã•ã‚Œã¾ã™ã€‚\", # 15\n",
    "    \"ã„ã„ãˆã€‚ä¸€æšç›®ã®ç”»åƒã¯{product_name1}ã§ã€äºŒæšç›®ã®ç”»åƒã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ç”»åƒAã¯{product_name1}ã§ã€ç”»åƒBã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å·¦ã®ç”»åƒã«ã¯{product_name1}ãŒã€å³ã®ç”»åƒã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã®å†™çœŸã«ã¯{product_name1}ãŒã€äºŒç•ªç›®ã®å†™çœŸã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã®ç”»åƒã®è£½å“ã¯{product_name1}ã§ã€äºŒç•ªç›®ã®ç”»åƒã®è£½å“ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å·¦å´ã¯{product_name1}ã§ã€å³å´ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã®å†™çœŸã¯{product_name1}ã‚’ã€äºŒç•ªç›®ã®å†™çœŸã¯{product_name2}ã‚’æå†™ã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚ç”»åƒAã«ã¯{product_name1}ãŒã€ç”»åƒBã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å·¦ã®å†™çœŸã«ã¯{product_name1}ãŒã€å³ã®å†™çœŸã«ã¯{product_name2}ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å†™çœŸAã¯{product_name1}ã‚’ã€å†™çœŸBã¯{product_name2}ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã®è£½å“ã¯{product_name1}ã§ã€äºŒç•ªç›®ã®è£½å“ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å·¦å´ã®ç”»åƒã¯{product_name1}ã§ã€å³å´ã®ç”»åƒã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã¯{product_name1}ã§ã€äºŒç•ªç›®ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚å·¦å´ã®è£½å“ã¯{product_name1}ã§ã€å³å´ã®è£½å“ã¯{product_name2}ã§ã™ã€‚\",\n",
    "    \"ã„ã„ãˆã€‚æœ€åˆã«{product_name1}ãŒè¦‹ã‚‰ã‚Œã€ãã®å¾Œã«{product_name2}ãŒç¶šãã¾ã™ã€‚\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def fill_instructions_and_answers(output_json_path, instructions, answers):\n",
    "    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(output_json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "\n",
    "    # \"flag\"ãŒ\"product_yes\"ã®é …ç›®ã ã‘ã‚’å‡¦ç†ã™ã‚‹\n",
    "    for key, value in data.items():\n",
    "        if value[\"flag\"] == \"product_yes\":\n",
    "            # \"image_ids\"ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€åãŒç•°ãªã‚‹ã‹ç¢ºèª\n",
    "            parent_folders = [image_id.split('+')[0].replace(\"_\", \" \") for image_id in value[\"image_ids\"]]\n",
    "            if parent_folders[0] != parent_folders[1]:\n",
    "                # instructions ã¨ answers ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "                question_index = random.randint(0, len(instructions) - 1)\n",
    "                value[\"instruction\"] = instructions[question_index]\n",
    "                value[\"answer\"] = answers[question_index].format(product_name1=parent_folders[0], product_name2=parent_folders[1])\n",
    "            else:\n",
    "                print(f\"Error: Image IDs in {key} have matching parent folder names.\")\n",
    "                break\n",
    "\n",
    "    # æ›´æ–°ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ä½¿ç”¨ä¾‹\n",
    "fill_instructions_and_answers(os.path.join(output_json_path, output_json_name), instructions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flags_in_json(json_path):\n",
    "    \"\"\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€'product_no' ã¨ 'product_yes' ã®å„ãƒ•ãƒ©ã‚°ã®åˆè¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        count_no = sum(1 for item in data.values() if item.get(\"flag\") == \"product_no\")\n",
    "        count_yes = sum(1 for item in data.values() if item.get(\"flag\") == \"product_yes\")\n",
    "    print(f\"product_no: {count_no}\")\n",
    "    print(f\"product_yes: {count_yes}\")\n",
    "    return count_no, count_yes\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "product_no, product_yes = count_flags_in_json(os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒãªã„å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é †ã«èµ°æŸ»\n",
    "    parent_folders = sorted([f.path for f in os.scandir(base_folder) if f.is_dir()])\n",
    "    for parent_folder in parent_folders:\n",
    "        child_folders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir()])\n",
    "\n",
    "        for child_folder in child_folders:\n",
    "            images = [img for img in os.listdir(child_folder) if is_image_file(img)]\n",
    "            pairs = []\n",
    "\n",
    "            if os.path.basename(child_folder).lower() == \"none\" and len(images) >= 2:\n",
    "                # \"None\"ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰2æšé¸æŠ\n",
    "                for _ in range(NUM_PAIRS):\n",
    "                    selected_images = random.sample(images, 2)\n",
    "                    pair = [f\"{os.path.basename(parent_folder)}+None+{os.path.splitext(img)[0]}\" for img in selected_images]\n",
    "                    pairs.append(pair)\n",
    "            elif len(images) >= 1:\n",
    "                # \"None\"ä»¥å¤–ã®ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰1æšé¸æŠã€åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ã‚‚ã†1æšé¸æŠ\n",
    "                for _ in range(NUM_PAIRS):\n",
    "                    selected_image = random.choice(images)\n",
    "                    first_image_key = f\"{os.path.basename(parent_folder)}+{os.path.basename(child_folder)}+{os.path.splitext(selected_image)[0]}\"\n",
    "\n",
    "                    other_child_folders = [f for f in child_folders if os.path.basename(f).lower() != \"none\"]\n",
    "                    if other_child_folders:\n",
    "                        other_folder = random.choice(other_child_folders)\n",
    "                        other_images = [img for img in os.listdir(other_folder) if is_image_file(img)]\n",
    "                        if other_images:\n",
    "                            selected_other_image = random.choice(other_images)\n",
    "                            second_image_key = f\"{os.path.basename(parent_folder)}+{os.path.basename(other_folder)}+{os.path.splitext(selected_other_image)[0]}\"\n",
    "                            pair = [first_image_key, second_image_key]\n",
    "                            if pair[0] != pair[1]:\n",
    "                                pairs.append(pair)\n",
    "\n",
    "            # é‡è¤‡ã‚’é™¤å»\n",
    "            unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "\n",
    "            # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "            for pair in unique_pairs:\n",
    "                existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "                    \"instruction\": \"\",\n",
    "                    \"answer\": \"\",\n",
    "                    \"image_ids\": pair,\n",
    "                    \"flag\": \"existence_same_no\"\n",
    "                }\n",
    "                next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œ\n",
    "NUM_PAIRS = 11  # 1ã¤ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒã‚ã‚‹å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é †ã«èµ°æŸ»\n",
    "    parent_folders = sorted([f.path for f in os.scandir(base_folder) if f.is_dir()])\n",
    "    for parent_folder in parent_folders:\n",
    "        child_folders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir()])\n",
    "        none_folder = [f for f in child_folders if os.path.basename(f).lower() == \"none\"]\n",
    "        other_folders = [f for f in child_folders if os.path.basename(f).lower() != \"none\"]\n",
    "\n",
    "        if none_folder and len(child_folders) > 1:\n",
    "            none_folder = none_folder[0]\n",
    "            none_images = [img for img in os.listdir(none_folder) if is_image_file(img)]\n",
    "\n",
    "            if len(none_images) >= 1:\n",
    "                for other_folder in other_folders:\n",
    "                    other_images = [img for img in os.listdir(other_folder) if is_image_file(img)]\n",
    "                    pairs = []\n",
    "\n",
    "                    if len(other_images) >= 1:\n",
    "                        # æœ€å¤§NUM_PAIRSå›ã®ãƒšã‚¢å½¢æˆã‚’è©¦ã¿ã‚‹\n",
    "                        for _ in range(NUM_PAIRS):\n",
    "                            selected_none_image = random.choice(none_images)\n",
    "                            selected_other_image = random.choice(other_images)\n",
    "                            pair = [\n",
    "                                f\"{os.path.basename(parent_folder)}+{os.path.basename(none_folder)}+{os.path.splitext(selected_none_image)[0]}\",\n",
    "                                f\"{os.path.basename(parent_folder)}+{os.path.basename(other_folder)}+{os.path.splitext(selected_other_image)[0]}\"\n",
    "                            ]\n",
    "                            random.shuffle(pair)\n",
    "                            pairs.append(pair)\n",
    "\n",
    "                        # é‡è¤‡ã‚’é™¤å»\n",
    "                        unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "\n",
    "                        # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "                        for pair in unique_pairs:\n",
    "                            existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "                                \"instruction\": \"\",\n",
    "                                \"answer\": \"\",\n",
    "                                \"image_ids\": pair,\n",
    "                                \"flag\": \"existence_same_yes\"\n",
    "                            }\n",
    "                            next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = 12  # 1ã¤ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flags_in_json(json_path):\n",
    "    \"\"\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€'product_no' ã¨ 'product_yes' ã®å„ãƒ•ãƒ©ã‚°ã®åˆè¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        count_no = sum(1 for item in data.values() if item.get(\"flag\") == \"existence_same_no\")\n",
    "        count_yes = sum(1 for item in data.values() if item.get(\"flag\") == \"existence_same_yes\")\n",
    "    print(f\"existence_same_no: {count_no}\")\n",
    "    print(f\"existence_same_yes: {count_yes}\")\n",
    "    return count_no, count_yes\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "existence_same_no, existence_same_yes = count_flags_in_json(os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒãªã„å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é †ã«èµ°æŸ»\n",
    "    parent_folders = sorted([f.path for f in os.scandir(base_folder) if f.is_dir()])\n",
    "    for parent_folder in parent_folders:\n",
    "        child_folders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir() and os.path.basename(f).lower() != \"none\"])\n",
    "        \n",
    "        for child_folder in child_folders:\n",
    "            images = [img for img in os.listdir(child_folder) if is_image_file(img)]\n",
    "            pairs = []\n",
    "\n",
    "            if len(images) >= 2:\n",
    "                # æœ€å¤§NUM_PAIRSå›ã®ãƒšã‚¢å½¢æˆã‚’è©¦ã¿ã‚‹\n",
    "                for _ in range(NUM_PAIRS):\n",
    "                    selected_images = random.sample(images, 2)\n",
    "                    pair = [f\"{os.path.basename(parent_folder)}+{os.path.basename(child_folder)}+{os.path.splitext(img)[0]}\" for img in selected_images]\n",
    "                    pairs.append(pair)\n",
    "\n",
    "            # é‡è¤‡ã‚’é™¤å»\n",
    "            unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "\n",
    "            # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "            for pair in unique_pairs:\n",
    "                existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "                    \"instruction\": \"\",\n",
    "                    \"answer\": \"\",\n",
    "                    \"image_ids\": pair,\n",
    "                    \"flag\": \"mode_same_no\"\n",
    "                }\n",
    "                next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = 20  # 1ã¤ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"Do these {product} images display different defect_modes?\",\n",
    "    \"In both {product} pictures, is there a variation in defect types?\",\n",
    "    \"Are the defects in these two {product} images varied?\",\n",
    "    \"For these {product} pictures, do the fault modes differ?\",\n",
    "    \"In these {product} photos, are the flaws manifested differently?\",\n",
    "    \"Do these {product} images exhibit diverse defect_modes?\",\n",
    "    \"Are the defect modes in both {product} pictures distinct?\",\n",
    "    \"In these two {product} images, is there a discrepancy in defect types?\",\n",
    "    \"Do these {product} photos show varied defects?\",\n",
    "    \"Are the types of defects in these {product} images different?\",\n",
    "    \"For these two {product} pictures, do the fault modes vary?\",\n",
    "    \"In both {product} images, are the flaws displayed differently?\",\n",
    "    \"Do these {product} pictures reveal distinct defect modes?\",\n",
    "    \"Are the defect modes in these {product} photos different?\",\n",
    "    \"In these {product} images, are the faults diverse?\", # 15\n",
    "    \"Are the defect modes in these {product} photos identical?\", \n",
    "    \"In both {product} images, is the defect type the same?\",\n",
    "    \"Do these {product} pictures exhibit the same defect mode?\",\n",
    "    \"Are the defects in these two {product} photos the same?\",\n",
    "    \"For these {product} images, are the faults manifested in the same way?\",\n",
    "    \"Is there uniformity in the defect modes across both {product} pictures?\",\n",
    "    \"Do these {product} photos display the same types of defects?\",\n",
    "    \"Are the defect modes in these two {product} images the same?\",\n",
    "    \"In these {product} pictures, are the flaws the same?\",\n",
    "    \"Do both {product} images show the same defect modes?\",\n",
    "    \"For these {product} photos, are the types of defects identical?\",\n",
    "    \"Are the defects in these {product} images the same in nature?\",\n",
    "    \"In these two {product} pictures, are the defect modes identical?\",\n",
    "    \"Do these {product} images reveal the same defects?\",\n",
    "    \"Are the defect modes in both {product} photos the same?\",\n",
    "]\n",
    "\n",
    "instructions_ja = [\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã§ã¯ã€ä¸è‰¯ãƒ¢ãƒ¼ãƒ‰ã«é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã®ä¸­ã§ã€ä¸è‰¯ã®ç¨®é¡ã«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®ç”»åƒã«ã¯ã€ç•°ãªã‚‹ä¸è‰¯ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ãŒç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ã¯ç•°ãªã‚‹æ–¹æ³•ã§è¡¨ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒé–“ã§ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã«å¤šæ§˜æ€§ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯ç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®ç”»åƒã§ã¯ã€æ¬ é™¥ã®ç¨®é¡ã«é•ã„ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€ç•°ãªã‚‹æ¬ é™¥ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã§ã¯ã€æ¬ é™¥ã®ç¨®é¡ãŒç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®å†™çœŸã§ã¯ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ãŒç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®ç”»åƒã§ã¯ã€æ¬ é™¥ãŒç•°ãªã‚‹æ–¹æ³•ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€ç•°ãªã‚‹æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯ç•°ãªã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã«ãŠã„ã¦ã€æ¬ é™¥ã¯å¤šæ§˜ã§ã™ã‹ï¼Ÿ\", # 15\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã®æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯åŒä¸€ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®ç”»åƒã«ãŠã„ã¦ã€ä¸è‰¯ã®ç¨®é¡ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€åŒã˜æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã§ã¯ã€æ¬ é™¥ã¯åŒã˜æ–¹æ³•ã§è¡¨ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã«ä¸€è²«æ€§ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€åŒã˜ç¨®é¡ã®æ¬ é™¥ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®ç”»åƒã§ã¯ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€æ¬ é™¥ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®ç”»åƒã§ã¯ã€åŒã˜æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®å†™çœŸã§ã¯ã€æ¬ é™¥ã®ç¨®é¡ã¯åŒä¸€ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã«ãŠã„ã¦ã€æ¬ é™¥ã¯åŒã˜æ€§è³ªã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰2æšã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯åŒä¸€ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ã“ã‚Œã‚‰ã®{product}ã®ç”»åƒã§ã¯ã€åŒã˜æ¬ é™¥ãŒæ˜ã‚‰ã‹ã«ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\",\n",
    "    \"ä¸¡æ–¹ã®{product}ã®å†™çœŸã«ãŠã„ã¦ã€æ¬ é™¥ãƒ¢ãƒ¼ãƒ‰ã¯åŒã˜ã§ã™ã‹ï¼Ÿ\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"No. The defect modes in these {product} photos are both {defect_mode}.\",\n",
    "    \"No. In both {product} images, the defect type is {defect_mode}.\",\n",
    "    \"No. These {product} pictures exhibit the {defect_mode}.\",\n",
    "    \"No. The defects in these two {product} photos are {defect_mode}.\",\n",
    "    \"No. For these {product} images, the faults are manifested as {defect_mode}.\",\n",
    "    \"No. There is uniformity in the defect modes across both {product} pictures, which is {defect_mode}.\",\n",
    "    \"No. These {product} photos display the {defect_mode}.\",\n",
    "    \"No. The defect modes in these two {product} images are {defect_mode}.\",\n",
    "    \"No. In these {product} pictures, the flaws are {defect_mode}.\",\n",
    "    \"No. Both {product} images show the {defect_mode}.\",\n",
    "    \"No. For these {product} photos, the types of defects are {defect_mode}.\",\n",
    "    \"No. The defects in these {product} images are {defect_mode} in nature.\",\n",
    "    \"No. In these two {product} pictures, the defect modes are {defect_mode}.\",\n",
    "    \"No. These {product} images reveal the {defect_mode}.\",\n",
    "    \"No. The defect modes in both {product} photos are {defect_mode}.\",\n",
    "    \"Yes. Both of these {product} images display the {defect_mode}.\",\n",
    "    \"Yes. In both {product} pictures, the defect type is {defect_mode}.\",\n",
    "    \"Yes. The defects in these two {product} images are {defect_mode}.\",\n",
    "    \"Yes. For these {product} pictures, the fault modes are {defect_mode}.\",\n",
    "    \"Yes. In these {product} photos, the flaws are manifested as {defect_mode}.\",\n",
    "    \"Yes. These {product} images exhibit the {defect_mode}.\",\n",
    "    \"Yes. The defect modes in both {product} pictures are {defect_mode}.\",\n",
    "    \"Yes. In these two {product} images, the defect types are {defect_mode}.\",\n",
    "    \"Yes. These {product} photos show the {defect_mode}.\",\n",
    "    \"Yes. The types of defects in these {product} images are {defect_mode}.\",\n",
    "    \"Yes. For these two {product} pictures, the fault modes are {defect_mode}.\",\n",
    "    \"Yes. In both {product} images, the flaws are {defect_mode}.\",\n",
    "    \"Yes. These {product} pictures reveal the {defect_mode}.\",\n",
    "    \"Yes. The defect modes in these {product} photos are {defect_mode}.\",\n",
    "    \"Yes. In these {product} images, the faults are {defect_mode}.\",\n",
    "]\n",
    "\n",
    "answers_ja = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒä¸€è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒã‚ã‚‹å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é †ã«èµ°æŸ»\n",
    "    parent_folders = sorted([f.path for f in os.scandir(base_folder) if f.is_dir()])\n",
    "    for parent_folder in parent_folders:\n",
    "        pairs = []\n",
    "        child_folders = sorted([f.path for f in os.scandir(parent_folder) if f.is_dir() and os.path.basename(f).lower() != \"none\"])\n",
    "        \n",
    "        if len(child_folders) >= 2:\n",
    "            for _ in range(NUM_PAIRS):\n",
    "                selected_child_folders = random.sample(child_folders, 2)\n",
    "                pair = []\n",
    "                for child_folder in selected_child_folders:\n",
    "                    images = [img for img in os.listdir(child_folder) if is_image_file(img)]\n",
    "                    if images:\n",
    "                        selected_image = random.choice(images)\n",
    "                        image_key = f\"{os.path.basename(parent_folder)}+{os.path.basename(child_folder)}+{os.path.splitext(selected_image)[0]}\"\n",
    "                        pair.append(image_key)\n",
    "                pairs.append(pair)\n",
    "\n",
    "            # é‡è¤‡ã‚’é™¤å»\n",
    "            unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "            \n",
    "            # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "            for pair in unique_pairs:\n",
    "                existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "                    \"instruction\": \"\",\n",
    "                    \"answer\": \"\",\n",
    "                    \"image_ids\": pair,\n",
    "                    \"flag\": \"mode_same_yes\"\n",
    "                }\n",
    "                next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = 40  # 1ã¤ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flags_in_json(json_path):\n",
    "    \"\"\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€'product_no' ã¨ 'product_yes' ã®å„ãƒ•ãƒ©ã‚°ã®åˆè¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        count_no = sum(1 for item in data.values() if item.get(\"flag\") == \"mode_same_no\")\n",
    "        count_yes = sum(1 for item in data.values() if item.get(\"flag\") == \"mode_same_yes\")\n",
    "    print(f\"mode_same_no: {count_no}\")\n",
    "    print(f\"mode_same_yes: {count_yes}\")\n",
    "    return count_no, count_yes\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "mode_same_no, mode_same_yes = count_flags_in_json(os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒãªã„å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "    parent_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    pairs = []\n",
    "\n",
    "    for _ in range(NUM_PAIRS):\n",
    "        # æœ€åˆã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "        parent_folder_1 = random.choice(parent_folders)\n",
    "        child_folders_1 = [f.path for f in os.scandir(parent_folder_1) if f.is_dir()]\n",
    "        if child_folders_1:\n",
    "            child_folder_1 = random.choice(child_folders_1)\n",
    "            images_1 = [img for img in os.listdir(child_folder_1) if is_image_file(img)]\n",
    "            if images_1:\n",
    "                selected_image_1 = random.choice(images_1)\n",
    "                first_image_key = f\"{os.path.basename(parent_folder_1)}+{os.path.basename(child_folder_1)}+{os.path.splitext(selected_image_1)[0]}\"\n",
    "\n",
    "                # 2ç•ªç›®ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ\n",
    "                if os.path.basename(child_folder_1).lower() == \"none\":\n",
    "                    # \"None\"ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ç•°ãªã‚‹è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ\n",
    "                    while True:\n",
    "                        parent_folder_2 = random.choice(parent_folders)\n",
    "                        if parent_folder_2 != parent_folder_1:\n",
    "                            child_folders_2 = [f.path for f in os.scandir(parent_folder_2) if f.is_dir() and os.path.basename(f).lower() == \"none\"]\n",
    "                            if child_folders_2:\n",
    "                                break\n",
    "                else:\n",
    "                    # \"None\"ä»¥å¤–ã®å­ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ç•°ãªã‚‹è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ\n",
    "                    while True:\n",
    "                        parent_folder_2 = random.choice(parent_folders)\n",
    "                        if parent_folder_2 != parent_folder_1:\n",
    "                            child_folders_2 = [f.path for f in os.scandir(parent_folder_2) if f.is_dir() and os.path.basename(f).lower() != \"none\"]\n",
    "                            if child_folders_2:\n",
    "                                break\n",
    "                            \n",
    "                child_folder_2 = random.choice(child_folders_2)\n",
    "                images_2 = [img for img in os.listdir(child_folder_2) if is_image_file(img)]\n",
    "                if images_2:\n",
    "                    selected_image_2 = random.choice(images_2)\n",
    "                    second_image_key = f\"{os.path.basename(parent_folder_2)}+{os.path.basename(child_folder_2)}+{os.path.splitext(selected_image_2)[0]}\"\n",
    "                    pair = [first_image_key, second_image_key]\n",
    "                    if pair[0] != pair[1]:\n",
    "                        pairs.append(pair)\n",
    "\n",
    "    # é‡è¤‡ã‚’é™¤å»\n",
    "    unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "\n",
    "    # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "    for pair in unique_pairs:\n",
    "        existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "            \"instruction\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"image_ids\": pair,\n",
    "            \"flag\": \"existence_different_no\"\n",
    "        }\n",
    "        next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = existence_same_no  # ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®æœ‰ç„¡ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒã‚ã‚‹å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
    "    parent_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    pairs = []\n",
    "\n",
    "    for _ in range(NUM_PAIRS):\n",
    "        # \"None\"ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "        parent_folder_none = random.choice(parent_folders)\n",
    "        child_folders_none = [f.path for f in os.scandir(parent_folder_none) if f.is_dir() and os.path.basename(f).lower() == \"none\"]\n",
    "        if not child_folders_none:\n",
    "            continue\n",
    "\n",
    "        none_folder = child_folders_none[0]\n",
    "        none_images = [img for img in os.listdir(none_folder) if is_image_file(img)]\n",
    "        if none_images:\n",
    "            selected_none_image = random.choice(none_images)\n",
    "            none_image_key = f\"{os.path.basename(parent_folder_none)}+None+{os.path.splitext(selected_none_image)[0]}\"\n",
    "\n",
    "            # \"None\"ä»¥å¤–ã®å­ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ç•°ãªã‚‹è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "            parent_folder_other = random.choice(parent_folders)\n",
    "            child_folders_other = [f.path for f in os.scandir(parent_folder_other) if f.is_dir() and os.path.basename(f).lower() != \"none\"]\n",
    "            if not child_folders_other:\n",
    "                continue\n",
    "\n",
    "            other_folder = random.choice(child_folders_other)\n",
    "            other_images = [img for img in os.listdir(other_folder) if is_image_file(img)]\n",
    "            if other_images:\n",
    "                selected_other_image = random.choice(other_images)\n",
    "                other_image_key = f\"{os.path.basename(parent_folder_other)}+{os.path.basename(other_folder)}+{os.path.splitext(selected_other_image)[0]}\"\n",
    "                pair = [none_image_key, other_image_key]\n",
    "                random.shuffle(pair)\n",
    "                # 2æšã®ç”»åƒãŒé¸ã°ã‚ŒãŸå ´åˆã€ãƒšã‚¢ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                if len(pair) == 2:\n",
    "                    pairs.append(pair)\n",
    "\n",
    "    # é‡è¤‡ã‚’é™¤å»\n",
    "    unique_pairs = [list(x) for x in set(tuple(x) for x in pairs)]\n",
    "    \n",
    "    # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "    for pair in unique_pairs:\n",
    "        existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "            \"instruction\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"image_ids\": pair,\n",
    "            \"flag\": \"existence_different_yes\"\n",
    "        }\n",
    "        next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = existence_same_yes  # 1ã¤ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ç”Ÿæˆã™ã‚‹ãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flags_in_json(json_path):\n",
    "    \"\"\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€'product_no' ã¨ 'product_yes' ã®å„ãƒ•ãƒ©ã‚°ã®åˆè¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        count_no = sum(1 for item in data.values() if item.get(\"flag\") == \"existence_different_no\")\n",
    "        count_yes = sum(1 for item in data.values() if item.get(\"flag\") == \"existence_different_yes\")\n",
    "    print(f\"existence_different_no: {count_no}\")\n",
    "    print(f\"existence_different_yes: {count_yes}\")\n",
    "    return count_no, count_yes\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "existence_different_no, existence_different_yes = count_flags_in_json(os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒãªã„å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
    "    parent_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    pairs = []\n",
    "\n",
    "    while len(pairs) < NUM_PAIRS:\n",
    "        # æœ€åˆã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "        parent_folder_1 = random.choice(parent_folders)\n",
    "        child_folders_1 = [f.path for f in os.scandir(parent_folder_1) if f.is_dir() and os.path.basename(f).lower() != \"none\"]\n",
    "        if child_folders_1:\n",
    "            child_folder_1 = random.choice(child_folders_1)\n",
    "            images_1 = [img for img in os.listdir(child_folder_1) if is_image_file(img)]\n",
    "            if images_1:\n",
    "                selected_image_1 = random.choice(images_1)\n",
    "                first_image_key = f\"{os.path.basename(parent_folder_1)}+{os.path.basename(child_folder_1)}+{os.path.splitext(selected_image_1)[0]}\"\n",
    "\n",
    "                # åŒã˜åå‰ã®å­ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ä»–ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã™\n",
    "                other_parent_folders = [pf for pf in parent_folders if pf != parent_folder_1 and os.path.exists(os.path.join(pf, os.path.basename(child_folder_1)))]\n",
    "                if other_parent_folders:\n",
    "                    parent_folder_2 = random.choice(other_parent_folders)\n",
    "                    child_folder_2 = os.path.join(parent_folder_2, os.path.basename(child_folder_1))\n",
    "                    images_2 = [img for img in os.listdir(child_folder_2) if is_image_file(img)]\n",
    "                    if images_2:\n",
    "                        selected_image_2 = random.choice(images_2)\n",
    "                        second_image_key = f\"{os.path.basename(parent_folder_2)}+{os.path.basename(child_folder_1)}+{os.path.splitext(selected_image_2)[0]}\"\n",
    "                        pair = [first_image_key, second_image_key]\n",
    "                        if pair not in pairs: # é‡è¤‡ã‚’é™¤å»\n",
    "                            pairs.append(pair)\n",
    "\n",
    "    # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "    for pair in pairs:\n",
    "        existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "            \"instruction\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"image_ids\": pair,\n",
    "            \"flag\": \"mode_different_no\"\n",
    "        }\n",
    "        next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = mode_same_no  # ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•°ãªã‚‹è£½å“ã«å¯¾ã—ã¦ã€å¤–è¦³çš„æ¬ é™¥ã®ãƒ¢ãƒ¼ãƒ‰ã®é•ã„ã‚’ç­”ãˆã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ (é•ã„ãŒã‚ã‚‹å ´åˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instructions_json(base_folder, output_json_path, NUM_PAIRS):\n",
    "    existing_data, next_instruction_id, _ = load_existing_json(output_json_path)\n",
    "\n",
    "    # è¦ªãƒ•ã‚©ãƒ«ãƒ€\n",
    "    parent_folders = [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "    pairs = []\n",
    "\n",
    "    while len(pairs) < NUM_PAIRS:\n",
    "        # æœ€åˆã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã¨\"None\"ä»¥å¤–ã®å­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "        parent_folder_1 = random.choice(parent_folders)\n",
    "        child_folders_1 = [f.path for f in os.scandir(parent_folder_1) if f.is_dir() and os.path.basename(f).lower() != \"none\"]\n",
    "        if child_folders_1:\n",
    "            child_folder_1 = random.choice(child_folders_1)\n",
    "            images_1 = [img for img in os.listdir(child_folder_1) if is_image_file(img)]\n",
    "            if images_1:\n",
    "                selected_image_1 = random.choice(images_1)\n",
    "                first_image_key = f\"{os.path.basename(parent_folder_1)}+{os.path.basename(child_folder_1)}+{os.path.splitext(selected_image_1)[0]}\"\n",
    "\n",
    "                # åŒã˜åå‰ã§ã¯ãªã„\"None\"ä»¥å¤–ã®å­ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚‹ä»–ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã™\n",
    "                other_parent_folders = [pf for pf in parent_folders if pf != parent_folder_1]\n",
    "                random.shuffle(other_parent_folders)\n",
    "                for opf in other_parent_folders:\n",
    "                    other_child_folders = [f.path for f in os.scandir(opf) if f.is_dir() and os.path.basename(f).lower() != \"none\" and os.path.basename(f) != os.path.basename(child_folder_1)]\n",
    "                    if other_child_folders:\n",
    "                        other_folder = random.choice(other_child_folders)\n",
    "                        other_images = [img for img in os.listdir(other_folder) if is_image_file(img)]\n",
    "                        if other_images:\n",
    "                            selected_image_2 = random.choice(other_images)\n",
    "                            second_image_key = f\"{os.path.basename(opf)}+{os.path.basename(other_folder)}+{os.path.splitext(selected_image_2)[0]}\"\n",
    "                            pair = [first_image_key, second_image_key]\n",
    "                            if pair not in pairs: # é‡è¤‡ã‚’é™¤å»\n",
    "                                pairs.append(pair)\n",
    "                                break\n",
    "\n",
    "    # JSONãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "    for pair in pairs:\n",
    "        existing_data[f\"SD_INS_{next_instruction_id}\"] = {\n",
    "            \"instruction\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"image_ids\": pair,\n",
    "            \"flag\": \"mode_different_yes\"\n",
    "        }\n",
    "        next_instruction_id += 1\n",
    "\n",
    "    # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump({\"data\": existing_data}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "NUM_PAIRS = mode_same_yes  # ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒšã‚¢æ•°\n",
    "create_instructions_json(base_folder, os.path.join(output_json_path, output_json_name), NUM_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructionã¨answerã‚’åŸ‹ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_flags_in_json(json_path):\n",
    "    \"\"\"JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€'product_no' ã¨ 'product_yes' ã®å„ãƒ•ãƒ©ã‚°ã®åˆè¨ˆã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)[\"data\"]\n",
    "        count_no = sum(1 for item in data.values() if item.get(\"flag\") == \"mode_different_no\")\n",
    "        count_yes = sum(1 for item in data.values() if item.get(\"flag\") == \"mode_different_yes\")\n",
    "    print(f\"mode_different_no: {count_no}\")\n",
    "    print(f\"mode_different_yes: {count_yes}\")\n",
    "    return count_no, count_yes\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "mode_different_no, mode_different_yes = count_flags_in_json(os.path.join(output_json_path, output_json_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def remove_ds_store(start_folder):\n",
    "#     for root, dirs, files in os.walk(start_folder):\n",
    "#         for file in files:\n",
    "#             if file == \".DS_Store\":\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 print(f\"Deleting {file_path}\")\n",
    "#                 os.remove(file_path)\n",
    "\n",
    "# # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦é–¢æ•°ã‚’å‘¼ã³å‡ºã™\n",
    "# remove_ds_store(\"./test3_check\")  # ã“ã“ã§'./'ã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ„å‘³ã—ã¾ã™ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
