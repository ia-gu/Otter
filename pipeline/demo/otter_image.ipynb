{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otter Image Demo (In-context Learning)\n",
    "\n",
    "Here is an example of multi-modal ICL (in-context learning) with ü¶¶ Otter. We provide two demo images with corresponding instructions and answers, then we ask the model to generate an answer given our instruct. You may change your instruction and see how the model responds.\n",
    "\n",
    "You can also try our [online demo](https://otter.cliangyu.com/) to see more in-context learning demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂøÖË¶Å„Å™„É¢„Ç∏„É•„Éº„É´„ÅØÂêÑËá™„Ç§„É≥„Çπ„Éà„Éº„É´<br>\n",
    "mlflow==2.6.0„ÅØ„Éê„Ç∞„Åå„ÅÇ„Çã„Åü„ÇÅ‰Ωø„Çè„Å™„ÅÑ„Åì„Å®(https://github.com/mlflow/mlflow/issues/9331) (2023.08.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade mlflow==2.5.0 pydantic==1.10.12 deepspeed==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ueno/.conda/envs/otter/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-24 05:30:33,801] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from otter.modeling_otter import OtterForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ë™≠„ÅøËæº„Åø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "The current model version is configured for Otter-Image with max_num_frames set to None.\n",
      "Total Trainable param: 1.385404 B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:14<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = OtterForConditionalGeneration.from_pretrained(\"luodian/OTTER-Image-MPT7B\", device_map=\"auto\") # Hugging Face\n",
    "model = OtterForConditionalGeneration.from_pretrained(\"/home/ueno/Otter/weights/OTTER-Image-MPT7B/\", device_map=\"auto\")\n",
    "tokenizer = model.text_tokenizer\n",
    "image_processor = transformers.CLIPImageProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### „Éà„Éº„ÇØ„É≥„ÅÆÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÁâπÊÆä„Éà„Éº„ÇØ„É≥\n",
    "model.text_tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [0, 50277, 50278, 50279, 50280]\n",
    "\n",
    "for i in lst:\n",
    "    print(f\"{i}: {tokenizer.decode(i)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_tokenizer.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_and_methods = dir(model.text_tokenizer)\n",
    "print(attributes_and_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(4374)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(2302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "arr = torch.tensor([-100, -100, -100, -100, 296, 404, 50277, 0, -100, -100])\n",
    "index = (arr != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "print(\"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ:\", index)\n",
    "\n",
    "# „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ5„ÅÆÂÄ§„ÇíÂèñÂæó\n",
    "value = arr[4].item()\n",
    "\n",
    "# ÂÄ§„ÇíÂá∫Âäõ\n",
    "print(\"„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ5„ÅÆÂÄ§:\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction_following.py„ÅÆtrain_one_epoch„ÅÆlabels\n",
    "sakai = [50343,  6989,    27,   769,   436,  2505,  6266,   247,  3295,    32,\n",
    "         4496,  4754,   310,   667,    13,    13,  4496,  2085,   247,  4278,\n",
    "          15,    15,   443,   627,    13,  4496,  3662,   594,    15, 50277,\n",
    "         5736,    27,  4374,  4374,  9370, 50277,     0, 50396, 50280, 50280]\n",
    "sakai = [    0, 50278,  6989,    27, 18566,   436,  2460,   452,   667, 12834,\n",
    "           32,   604,   627,   403,   667, 12834,    13,  4496,  2085,   253,\n",
    "         7071,  1416,    15,   604,   417,    13,  4496,  1333,  5293,    15,\n",
    "          443,  5736,    27, 50279,  2302,  8256, 50277,     0, 50280, 50280]\n",
    "# modeling_mpt.py„ÅÆclass MPTForCausalLM(MPTPreTrainedModel):„ÅÆforward\n",
    "# loss„Å´ÂÖ•„ÇãGT„ÅÆÂèØË¶ñÂåñ\n",
    "# sakai = [-100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          2302,    15,   831, 23069,  1057,   417,   452,   667, 12834,   824,\n",
    "#           347, 31806, 23069,    13, 28290, 23069,    13,   439,  6321,  3612,\n",
    "#         23069,    13, 13968, 23069,    13,   285, 15070, 23069,    13,   594,\n",
    "#           352,   310,  1327,    14,   615,   738,   422,    15, 50277,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  4374,\n",
    "#            15,   831, 23069,   556,   690, 31806, 23069,    13,   594,   352,\n",
    "#           310, 22327,    15, 50277,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  2302,    15,   831, 23069,  1057,   417,\n",
    "#           452,   667, 12834,   824,   347, 31806, 23069,    13, 28290, 23069,\n",
    "#            13,   439,  6321,  3612, 23069,    13, 13968, 23069,    13,   285,\n",
    "#         15070, 23069,    13,   594,   352,   310,  1327,    14,   615,   738,\n",
    "#           422,    15, 50277,     0,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "#          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]\n",
    "# # loss„Å´ÂÖ•„Çã„É¢„Éá„É´„ÅÆÂá∫Âäõ„ÅÆÂèØË¶ñÂåñ\n",
    "# sakai = [50343,  6989,    27,   769,   694,   247,  2460,   273,   247,    15,\n",
    "#          4496,   352,  3761,   452,   247,  3102,   390,   347, 11385,  2739,\n",
    "#           390, 11385, 23069,    13,   390,   357,  3612, 23069,    13,   390,\n",
    "#         23069,    13,   390, 15070, 23069,    32,  4496,  5736,    27,  1621,\n",
    "#          1621,    13, 50277,   310,  1057,   417,   452,   667,   273,    15,\n",
    "#           347, 31806, 23069,    13, 28290, 23069,    13,   439,  6321,  3612,\n",
    "#         23069,    13, 13968, 23069,    13,   390, 15070, 23069,    15,   285,\n",
    "#           352,   310,   275,    14,   615,   738,   422,    15, 50277,     0,\n",
    "#          6989,    27,   752,   310,   271,  2460,   273,   247,    15,  1057,\n",
    "#           436, 23069,   452,   667, 12834,   824,   347, 31806, 23069,    13,\n",
    "#         28290, 23069,    13,   439,  6321,  3612, 23069,    13, 13968, 23069,\n",
    "#            13,   285, 15070, 23069,    32,   443,  5736,    27,  1621,  1621,\n",
    "#            13,   380, 23069,   556,   247, 12834, 12834, 12834, 28290,   352,\n",
    "#           310,   417,    15, 50277,     0,  6989,    27,   752,   310,   271,\n",
    "#          2460,   273,   247,    15,   752,   436, 23069,   452,   667, 12834,\n",
    "#           824,   347, 31806, 23069,    13, 28290, 23069,    13,   439,  6321,\n",
    "#          3612, 23069,    13, 13968, 23069,    13,   285, 15070, 23069,    32,\n",
    "#           443,  5736,    27,  6279,  6279,    13,   831, 23069,  1057,   417,\n",
    "#           452,   667, 12834,   824,   347, 31806, 23069,    13, 28290, 23069,\n",
    "#            13,   439,  6321,  3612, 23069,    13, 13968, 23069,    13,   285,\n",
    "#         15070, 23069,    13,   594,   352,   310,  1327,    14,   615,   738,\n",
    "#           422,    15, 50277,     0, 50396, 50280, 50280, 50280, 50280, 50280,\n",
    "#         50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280,\n",
    "#         50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280,\n",
    "#         50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280,\n",
    "#         50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280, 50280]\n",
    "# sakai = [50280 if x == -100 else x for x in sakai]\n",
    "print(sakai)\n",
    "for s in sakai:\n",
    "    print(tokenizer.decode(s),end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Êé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "demo_image_two = Image.open(requests.get(\"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\", stream=True).raw)\n",
    "query_image = Image.open(requests.get(\"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", stream=True).raw)\n",
    "\n",
    "vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "model.text_tokenizer.padding_side = \"left\"\n",
    "lang_x = model.text_tokenizer(\n",
    "    [\n",
    "        \"<image>User: a photo of GPT:<answer> two cats sleeping.<|endofchunk|><image>User: a photo of GPT:<answer> a bathroom sink.<|endofchunk|><image>User: a photo of GPT:<answer>\"\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(vision_x.shape) # torch.Size([1, 3, 1, 3, 224, 224]) shape (B, num_imgs, Frames=1, C, H, W)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].imshow(demo_image_one)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(demo_image_two)\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(query_image)\n",
    "axes[2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get the data type from model's parameters\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "# Convert tensors to the model's data type\n",
    "vision_x = vision_x.to(dtype=model_dtype)\n",
    "lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x.to(model.device),\n",
    "    lang_x=lang_x_input_ids.to(model.device),\n",
    "    attention_mask=lang_x_attention_mask.to(model.device),\n",
    "    max_new_tokens=512,\n",
    "    num_beams=3,\n",
    "    no_repeat_ngram_size=3,\n",
    "    bad_words_ids=bad_words_id,\n",
    ")\n",
    "\n",
    "parsed_output = (\n",
    "    model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-IT„ÅÆjson„Éï„Ç°„Ç§„É´„ÅÆÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã_instructions.json\n",
    "\n",
    "import orjson\n",
    "\n",
    "mimicit_path=\"../../data/LA/LACR_I2I_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    dataset = orjson.loads(f.read())\n",
    "    # dataset = orjson.loads(f.read())[\"data\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã.json\n",
    "\n",
    "import ijson\n",
    "\n",
    "images = {}\n",
    "images_path=\"../../data/LA/LA.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Åã„ÇâÁîªÂÉèÂèØË¶ñÂåñ\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# base64„Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Éá„Éº„Çø\n",
    "str_data1 = images[\"LA_IMG_000000215677\"]\n",
    "str_data2 = images[\"LA_IMG_000000429446\"]\n",
    "\n",
    "# „Éê„Ç§„Éà„Éá„Éº„Çø„Å´„Éá„Ç≥„Éº„Éâ\n",
    "decoded_data1 = base64.b64decode(str_data1)\n",
    "decoded_data2 = base64.b64decode(str_data2)\n",
    "\n",
    "# „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "image1 = Image.open(BytesIO(decoded_data1))\n",
    "image2 = Image.open(BytesIO(decoded_data2))\n",
    "\n",
    "# 2x1„ÅÆsubplot„Çí‰ΩúÊàê„Åó„Å¶„ÄÅ2Êûö„ÅÆÁîªÂÉè„ÇíË°®Á§∫\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "\n",
    "axarr[0].imshow(image1)\n",
    "axarr[0].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "axarr[1].imshow(image2)\n",
    "axarr[1].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚óã‚óã_train.json\n",
    "\n",
    "import orjson\n",
    "\n",
    "train_config_path=\"../../data/LA/LACR_I2I_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "cache_train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_list = list(cache_train_config.keys())\n",
    "print(len(cache_train_list))\n",
    "print(cache_train_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['LACR_I2I_INS_000000296754']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['LACR_I2I_INS_000000222475']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëá™‰Ωú„Éá„Éº„Çø„Çª„ÉÉ„ÉàÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/VI/train_VI_long_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    dataset = orjson.loads(f.read())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/VI/train_VI.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# base64„Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊñáÂ≠óÂàó„Éá„Éº„Çø\n",
    "str_data1 = images[\"metal+metal+image_55\"]\n",
    "str_data2 = images[\"metal+metal_rust+image_8\"]\n",
    "\n",
    "# „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "image1 = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "image2 = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "\n",
    "# 2x1„ÅÆsubplot„Çí‰ΩúÊàê„Åó„Å¶„ÄÅ2Êûö„ÅÆÁîªÂÉè„ÇíË°®Á§∫\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "\n",
    "axarr[0].imshow(image1)\n",
    "axarr[0].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "axarr[1].imshow(image2)\n",
    "axarr[1].axis('off')  # Ëª∏„ÇíÈùûË°®Á§∫„Å´\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/VI/train_VI_pairs25_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "cache_train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_list = list(cache_train_config.keys())\n",
    "print(len(cache_train_list))\n",
    "print(cache_train_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_train_config['book+aged_book+image_2=0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëá™‰Ωú„Éá„Éº„Çø„Çª„ÉÉ„ÉàÈáç„ÅøÊÄßËÉΩË™øÊüª(Ê¨†Èô•ÂêçÂΩì„Å¶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"context_true\"\n",
    "trained_ckpt_path = f'../../log/{model_name}/final_weights.pt'\n",
    "\n",
    "train_ckpt = torch.load(trained_ckpt_path, map_location=\"cpu\")\n",
    "if train_ckpt.get(\"model_state_dict\", None) is not None:\n",
    "    train_ckpt = train_ckpt[\"model_state_dict\"]\n",
    "_ = model.load_state_dict(train_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/AC/AC_train.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/AC/AC_train_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/AC/AC_train_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠£Ëß£Áéá\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "count = 0\n",
    "NUM = 10\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    query = keys[i].split('=')[0]\n",
    "    str_data3 = images[query]\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 10\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "    axes.imshow(query_image)\n",
    "    axes.axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/AC/AC_val.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/AC/AC_val_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/AC/AC_val_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠£Ëß£Áéá\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "count = 0\n",
    "NUM = 1000\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    query = keys[i].split('=')[0]\n",
    "    str_data3 = images[query]\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 10\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "    axes.imshow(query_image)\n",
    "    axes.axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"../../data/VI_test_jsons/VI_test.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"../../data/VI_test_jsons/VI_test_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"../../data/VI_test_jsons/VI_test_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠£Ëß£Áéá\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "count = 0\n",
    "total_ok = 0\n",
    "total_ng = 0\n",
    "count_ok = 0\n",
    "count_ng = 0\n",
    "miss_ok = []\n",
    "miss_ng = []\n",
    "NUM = 1000\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    query = keys[i].split('=')[0]\n",
    "    str_data3 = images[query]\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    # if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "    #     count += 1\n",
    "    if query.split('+')[1]==\"None\":\n",
    "        total_ok += 1\n",
    "        if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "            count_ok += 1\n",
    "        else:\n",
    "            miss_ok.append(query)\n",
    "    else:\n",
    "        total_ng += 1\n",
    "        if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "            count_ng += 1\n",
    "        else:\n",
    "            miss_ng.append(query)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "# print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")\n",
    "print(f\"ok correct: {count_ok}, total: {total_ok}, acc: {(count_ok / total_ok) * 100:.2f}%\")\n",
    "print(f\"ng correct: {count_ng}, total: {total_ng}, acc: {(count_ng / total_ng) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 400\n",
    "count = 0\n",
    "keys = [s for s in keys if s.split('+')[1] != \"None\"]\n",
    "# for i in range(len(keys[100:NUM])):\n",
    "for i, key in enumerate(keys[300:NUM]):\n",
    "    query = key.split('=')[0]\n",
    "\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User: {instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "    axes.imshow(query_image)\n",
    "    axes.axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {i+1}, acc: {(count / (i+1)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVTecAD(Ê¨†Èô•ÂêçÂΩì„Å¶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_image_paths(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ÁîªÂÉè„ÅÆÊã°ÂºµÂ≠ê„É™„Çπ„Éà\n",
    "    all_files = sorted(os.listdir(folder_path)) # „Éï„Ç©„É´„ÉÄÂÜÖ„ÅÆÂÖ®„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó\n",
    "    image_paths = [os.path.join(folder_path, file) for file in all_files if os.path.splitext(file)[1].lower() in image_extensions] # ÁîªÂÉè„ÅÆ„Éë„Çπ„ÇíÊäΩÂá∫„Åó„Å¶„É™„Çπ„Éà„Å´Ê†ºÁ¥ç\n",
    "    return image_paths\n",
    "\n",
    "def write_text_file(file_path, text):\n",
    "    with open(file_path, mode=\"a\") as f:\n",
    "        f.write(text+\"\\n\")\n",
    "        \n",
    "def generate_list_string(items):\n",
    "    # „Ç¢„É≥„ÉÄ„Éº„Çπ„Ç≥„Ç¢„Çí„Çπ„Éö„Éº„Çπ„Å´Â§âÊèõ\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÇØ„Ç®„É™„ÅÆ„Åø‰ΩøÁî®\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def test(folder, sub_folder, GTs, model_name):\n",
    "    acc = []\n",
    "    for j, (sub,gt) in enumerate(zip(sub_folder,GTs)):\n",
    "        folder_name = f'./result/{folder}/{sub}/{model_name}'\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        with open(f'{folder_name}/AC.txt', mode='w') as f:\n",
    "            f.close()\n",
    "        \n",
    "        model.text_tokenizer.padding_side = \"left\"\n",
    "        \n",
    "        sentence = f\"{sub} --> {gt}\"\n",
    "        # print(sentence)\n",
    "        write_text_file(f'{folder_name}/AC.txt',sentence)\n",
    "        inputs = textwrap.dedent(f\"\"\"\n",
    "           <image>User: What are the defects present in this image? If there are none, please say None. GPT:<answer>\n",
    "        \"\"\")    \n",
    "        \n",
    "        inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "        lang_x = model.text_tokenizer(\n",
    "            [\n",
    "                inputs\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        write_text_file(f'{folder_name}/AC.txt',f'-----{sub} start-----')\n",
    "        write_text_file(f'{folder_name}/AC.txt',\"\")\n",
    "            \n",
    "        query_folder_path = f\"/data/dataset/mvtec/{folder}/test/{sub}\"\n",
    "        query_image_paths = get_image_paths(query_folder_path)\n",
    "        count = 0\n",
    "        for i, query_image_path in enumerate(query_image_paths[:]):\n",
    "            # print(query_image_path)\n",
    "            query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "            vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "            # Get the data type from model's parameters\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "            # Convert tensors to the model's data type\n",
    "            vision_x = vision_x.to(dtype=model_dtype)\n",
    "            lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "            lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "            bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x.to(model.device),\n",
    "                lang_x=lang_x_input_ids.to(model.device),\n",
    "                attention_mask=lang_x_attention_mask.to(model.device),\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                no_repeat_ngram_size=3,\n",
    "                bad_words_ids=bad_words_id,\n",
    "            )\n",
    "\n",
    "            parsed_output = (\n",
    "                model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "            )\n",
    "            \n",
    "            if parsed_output.lower()==gt.lower():\n",
    "                count += 1\n",
    "            \n",
    "            write_text_file(f'{folder_name}/AC.txt',query_image_path)\n",
    "            write_text_file(f'{folder_name}/AC.txt',parsed_output)\n",
    "            write_text_file(f'{folder_name}/AC.txt',\"\")\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # print(inputs)\n",
    "            # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "            # fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "            # axes.imshow(query_image)\n",
    "            # axes.axis('off')\n",
    "            # plt.show()\n",
    "            \n",
    "        accuracy = f\"correct: {count}, total: {len(query_image_paths)}, acc: {(count / (len(query_image_paths))) * 100:.2f}%\"\n",
    "        acc.append((sub,accuracy))\n",
    "        \n",
    "        write_text_file(f'{folder_name}/AC.txt',f'-----{sub} end-----')\n",
    "        write_text_file(f'{folder_name}/AC.txt',accuracy)\n",
    "\n",
    "    for a in acc:\n",
    "        print(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"bottle\"\n",
    "sub_folder = [\"good\",\"broken_large\",\"broken_small\",\"contamination\"]\n",
    "GTs = ['None','broken','broken','contamination']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"cable\"\n",
    "sub_folder = [\"good\",\"bent_wire\",\"cable_swap\",\"cut_inner_insulation\",\"cut_outer_insulation\",\"missing_cable\",\"missing_wire\",\"poke_insulation\"]\n",
    "GTs = [\"None\",'bent','swapp','crack','crack','missing','missing','hole']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"capsule\"\n",
    "sub_folder = [\"good\",\"crack\",\"faulty_imprint\",\"poke\",\"scratch\",\"squeeze\"]\n",
    "GTs = [\"None\",'crack','misprint','hole','scratch','misshapen']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"carpet\"\n",
    "sub_folder = [\"good\",\"color\",\"cut\",\"hole\",\"metal_contamination\",\"thread\"]\n",
    "GTs = [\"None\",'stain','cut','hole','contamination','contamination']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"grid\"\n",
    "sub_folder = [\"good\",\"bent\",\"broken\",\"glue\",\"metal_contamination\",\"thread\"]\n",
    "GTs = [\"None\",\"bent\",\"broken\",\"contamination\",\"contamination\",\"contamination\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"hazelnut\"\n",
    "sub_folder = [\"good\",\"crack\",\"cut\",\"hole\",\"print\"]\n",
    "GTs = ['None','crack','scratch','hole','misprint']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"leather\"\n",
    "sub_folder = [\"good\",\"color\",\"cut\",\"fold\",\"glue\",\"poke\"]\n",
    "GTs = ['None','stain','scratch','wrinkle','poked leather']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"metal_nut\"\n",
    "sub_folder = [\"good\",\"bent\",\"color\",\"flip\",\"scratch\"]\n",
    "GTs = ['None','bent','stain','flip','scratch']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"pill\"\n",
    "sub_folder = [\"good\",\"color\",\"contamination\",\"crack\",\"faulty_imprint\",\"scratch\",\"pill_type\"]\n",
    "GTs = ['None','stain','contamination','crack','misprint','scratch','stain']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"screw\"\n",
    "sub_folder = [\"good\",\"manipulated_front\",\"scratch_head\",\"scratch_neck\",\"thread_side\",\"thread_top\"]\n",
    "GTs = ['None','strip','chip','chip','chip','chip']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"tile\"\n",
    "sub_folder = [\"good\",\"crack\",\"glue_strip\",\"gray_stroke\",\"oil\",\"rough\"]\n",
    "GTs = ['None','crack','contamination','stain','stain','contamination']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"toothbrush\"\n",
    "sub_folder = [\"good\",\"defective\"]\n",
    "GTs = [\"None\",\"broken\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"transistor\"\n",
    "sub_folder = [\"good\",\"bent_lead\",\"cut_lead\",\"damaged_case\",\"misplaced\"]\n",
    "GTs = [\"None\",\"bent\",\"cut\",\"broken\",\"misalignment\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"wood\"\n",
    "sub_folder = [\"good\",\"color\",\"scratch\",\"liquid\",\"hole\"]\n",
    "GTs = [\"None\",\"stain\",\"scratch\",\"stain\",\"hole\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"zipper\"\n",
    "sub_folder = [\"good\",\"broken_teeth\",\"fabric_border\",\"fabric_interior\",\"rough\",\"split_teeth\",\"squeezed_teeth\"]\n",
    "GTs = [\"None\",\"broken\",\"tear\",\"frayed\",\"frayed\",\"misshapen\",\"misshapen\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÈùûÂÖ¨Èñã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category = \"rice\"\n",
    "# anormaly_reason = [\"brokened rice\",\"rice with milky white\"]\n",
    "# anormaly_type = [\"broken\",\"milky_white\"]\n",
    "\n",
    "# test(category, anormaly_reason, anormaly_type, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëá™‰Ωú„Éá„Éº„Çø„Çª„ÉÉ„ÉàÈáç„ÅøÊÄßËÉΩË™øÊüª(„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"AC-VI_loss_context_query/batch128_epoch1_lr-5_pairs25_weight5\"\n",
    "model_name = \"context_true\"\n",
    "# model_name = \"debug\"\n",
    "# trained_ckpt_path = f'/home/yyamada/Otter_/log/VI_batch128_long_pairs25/final_weights.pt'\n",
    "trained_ckpt_path = f'../../log/{model_name}/final_weights.pt'\n",
    "\n",
    "train_ckpt = torch.load(trained_ckpt_path, map_location=\"cpu\")\n",
    "if train_ckpt.get(\"model_state_dict\", None) is not None:\n",
    "    train_ckpt = train_ckpt[\"model_state_dict\"]\n",
    "_ = model.load_state_dict(train_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_train.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_train_pairs25_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_train_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠£Ëß£Áéá\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "yesno_count = 0\n",
    "reason_count = 0\n",
    "both_count = 0\n",
    "NUM = 500\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "    \n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "    \n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\" \")[0].lower()==parsed_output.split(\" \")[0].lower():\n",
    "        yesno_count += 1\n",
    "        if len(parsed_output.split(\" \")) > 1:\n",
    "            if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                both_count += 1\n",
    "    if len(parsed_output.split(\" \")) > 1:\n",
    "        if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                reason_count += 1\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "print(f\"yesno correct: {yesno_count}, total: {NUM}, acc: {(yesno_count / NUM) * 100:.2f}%\")\n",
    "print(f\"reason correct: {reason_count}, total: {NUM}, acc: {(reason_count / NUM) * 100:.2f}%\")\n",
    "print(f\"both correct: {both_count}, total: {NUM}, acc: {(both_count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "yesno_count = 0\n",
    "reason_count = 0\n",
    "both_count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\" \")[0].lower()==parsed_output.split(\" \")[0].lower():\n",
    "        yesno_count += 1\n",
    "        if len(parsed_output.split(\" \")) > 1:\n",
    "            if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                both_count += 1\n",
    "    if len(parsed_output.split(\" \")) > 1:\n",
    "        if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                reason_count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"yesno correct: {yesno_count}, total: {NUM}, acc: {(yesno_count / NUM) * 100:.2f}%\")\n",
    "print(f\"reason correct: {reason_count}, total: {NUM}, acc: {(reason_count / NUM) * 100:.2f}%\")\n",
    "print(f\"both correct: {both_count}, total: {NUM}, acc: {(both_count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val„Éá„Éº„Çø„ÅßÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "import ijson\n",
    "import orjson\n",
    "\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_val.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_val_pairs1_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/VI_jsons/VI_val_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠£Ëß£Áéá\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "yesno_count = 0\n",
    "reason_count = 0\n",
    "both_count = 0\n",
    "NUM = 500\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "    \n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "    \n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\" \")[0].lower()==parsed_output.split(\" \")[0].lower():\n",
    "        yesno_count += 1\n",
    "        if len(parsed_output.split(\" \")) > 1:\n",
    "            if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                both_count += 1\n",
    "    if len(parsed_output.split(\" \")) > 1:\n",
    "        if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                reason_count += 1\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "print(f\"yesno correct: {yesno_count}, total: {NUM}, acc: {(yesno_count / NUM) * 100:.2f}%\")\n",
    "print(f\"reason correct: {reason_count}, total: {NUM}, acc: {(reason_count / NUM) * 100:.2f}%\")\n",
    "print(f\"both correct: {both_count}, total: {NUM}, acc: {(both_count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "yesno_count = 0\n",
    "reason_count = 0\n",
    "both_count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].split(\" \")[0].lower()==parsed_output.split(\" \")[0].lower():\n",
    "        yesno_count += 1\n",
    "        if len(parsed_output.split(\" \")) > 1:\n",
    "            if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                both_count += 1\n",
    "    if len(parsed_output.split(\" \")) > 1:\n",
    "        if instructions[\"data\"][query][\"answer\"].split(\" \")[1].lower()==parsed_output.split(\" \")[1].lower():\n",
    "                reason_count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"yesno correct: {yesno_count}, total: {NUM}, acc: {(yesno_count / NUM) * 100:.2f}%\")\n",
    "print(f\"reason correct: {reason_count}, total: {NUM}, acc: {(reason_count / NUM) * 100:.2f}%\")\n",
    "print(f\"both correct: {both_count}, total: {NUM}, acc: {(both_count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVTecAD(„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_image_paths(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']  # ÁîªÂÉè„ÅÆÊã°ÂºµÂ≠ê„É™„Çπ„Éà\n",
    "    all_files = sorted(os.listdir(folder_path)) # „Éï„Ç©„É´„ÉÄÂÜÖ„ÅÆÂÖ®„Å¶„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó\n",
    "    image_paths = [os.path.join(folder_path, file) for file in all_files if os.path.splitext(file)[1].lower() in image_extensions] # ÁîªÂÉè„ÅÆ„Éë„Çπ„ÇíÊäΩÂá∫„Åó„Å¶„É™„Çπ„Éà„Å´Ê†ºÁ¥ç\n",
    "    return image_paths\n",
    "\n",
    "def write_text_file(file_path, text):\n",
    "    with open(file_path, mode=\"a\") as f:\n",
    "        f.write(text+\"\\n\")\n",
    "        \n",
    "def generate_list_string(items):\n",
    "    # „Ç¢„É≥„ÉÄ„Éº„Çπ„Ç≥„Ç¢„Çí„Çπ„Éö„Éº„Çπ„Å´Â§âÊèõ\n",
    "    items = [item.replace('_', ' ') for item in items]\n",
    "    if len(items) == 1:\n",
    "        return items[0]\n",
    "    elif len(items) == 2:\n",
    "        return f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        return \", \".join(items[:-1]) + f\", and {items[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Åø‰ΩøÁî®\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def test(folder, sub_folder, GTs, model_name, order=True):\n",
    "    acc = []\n",
    "    if folder==\"grid\":\n",
    "        folder__ = \"metal grid\"\n",
    "    else:\n",
    "        folder__ = folder\n",
    "    folder__ = folder__.replace('_', ' ')\n",
    "    for sub,gt in zip(sub_folder,GTs):\n",
    "        folder_name = f'./result/{folder}/{sub}/{model_name}'\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        with open(f'{folder_name}/detective.txt', mode='w') as f:\n",
    "            f.close()\n",
    "        with open(f'{folder_name}/non-detective.txt', mode='w') as f:\n",
    "            f.close()\n",
    "        \n",
    "        subfolder_string = generate_list_string(GTs)\n",
    "        model.text_tokenizer.padding_side = \"left\"\n",
    "        sentence = f\"{sub} --> {gt}\"\n",
    "        write_text_file(f'{folder_name}/detective.txt',sentence)\n",
    "        \n",
    "        \"\"\" „ÇØ„Ç®„É™Ôºö‰∏çËâØÂìÅ \"\"\"\n",
    "        if order: # demo_image_one: ËâØÂìÅ, demo_image_two: ‰∏çËâØÂìÅ\n",
    "            sentence = f\"context1: OK, context2: NG, query: NG\"\n",
    "            write_text_file(f'{folder_name}/detective.txt',sentence)\n",
    "            demo_image_one = Image.open(f\"/data/dataset/mvtec/{folder}/test/good/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            demo_image_two = Image.open(f\"/data/dataset/mvtec/{folder}/test/{sub}/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            \n",
    "            inputs = textwrap.dedent(f\"\"\"\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>No None<|endofchunk|>\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>Yes {gt}<|endofchunk|>\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>\n",
    "            \"\"\")\n",
    "        \n",
    "        else: # demo_image_one: ‰∏çËâØÂìÅ, demo_image_two: ËâØÂìÅ\n",
    "            sentence = f\"context1: NG, context2: OK, query: NG\"\n",
    "            write_text_file(f'{folder_name}/detective.txt',sentence)\n",
    "            demo_image_one = Image.open(f\"/data/dataset/mvtec/{folder}/test/{sub}/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            demo_image_two = Image.open(f\"/data/dataset/mvtec/{folder}/test/good/000.png\").resize((224, 224)).convert(\"RGB\")\n",
    "            \n",
    "            inputs = textwrap.dedent(f\"\"\"\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>Yes {gt}<|endofchunk|>\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>No None<|endofchunk|>\n",
    "                <image>User: This is an image of {folder__}. Does this {folder__} have any defects such as {subfolder_string}? If there are any defects, please provide the defect name. If not, please say None. GPT:<answer>\n",
    "            \"\"\")\n",
    "        \n",
    "        inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "        lang_x = model.text_tokenizer(\n",
    "            [\n",
    "                inputs\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        write_text_file(f'{folder_name}/detective.txt',f'-----{sub} start-----')\n",
    "        write_text_file(f'{folder_name}/detective.txt',\"\")\n",
    "            \n",
    "        query_folder_path = f\"/data/dataset/mvtec/{folder}/test/{sub}\"\n",
    "        query_image_paths = get_image_paths(query_folder_path)\n",
    "        yesno_count = 0\n",
    "        reason_count = 0\n",
    "        both_count = 0\n",
    "        for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "            # print(query_image_path)\n",
    "            query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "            vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "            # Get the data type from model's parameters\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "            # Convert tensors to the model's data type\n",
    "            vision_x = vision_x.to(dtype=model_dtype)\n",
    "            lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "            lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "            bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x.to(model.device),\n",
    "                lang_x=lang_x_input_ids.to(model.device),\n",
    "                attention_mask=lang_x_attention_mask.to(model.device),\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                no_repeat_ngram_size=3,\n",
    "                bad_words_ids=bad_words_id,\n",
    "            )\n",
    "\n",
    "            parsed_output = (\n",
    "                model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "            )\n",
    "            \n",
    "            print(inputs)\n",
    "            print(parsed_output)\n",
    "            \n",
    "            if parsed_output.split(\" \")[0].lower()==\"yes\":\n",
    "                yesno_count += 1\n",
    "                if len(parsed_output.split(\" \")) > 1:\n",
    "                    if parsed_output.split(\" \")[1].lower()==f\"{gt}\":\n",
    "                        both_count += 1\n",
    "            if len(parsed_output.split(\" \")) > 1:\n",
    "                if parsed_output.split(\" \")[1].lower()==f\"{gt}\":\n",
    "                        reason_count += 1\n",
    "                    \n",
    "            write_text_file(f'{folder_name}/detective.txt',query_image_path)\n",
    "            write_text_file(f'{folder_name}/detective.txt',parsed_output)\n",
    "            write_text_file(f'{folder_name}/detective.txt',\"\")\n",
    "            \n",
    "            # print(inputs)\n",
    "            # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "            # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "            # axes[0].imshow(demo_image_one)\n",
    "            # axes[0].axis('off')\n",
    "            # axes[1].imshow(demo_image_two)\n",
    "            # axes[1].axis('off')\n",
    "            # axes[2].imshow(query_image)\n",
    "            # axes[2].axis('off')\n",
    "            # plt.show()\n",
    "            print(i,sub)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        yesno_acc = f\"correct: {yesno_count}, total: {len(query_image_paths)-1}, yesno acc: {(yesno_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(yesno_acc)\n",
    "        reason_acc = f\"correct: {reason_count}, total: {len(query_image_paths)-1}, reason acc: {(reason_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(reason_acc)\n",
    "        both_acc = f\"correct: {both_count}, total: {len(query_image_paths)-1}, both acc: {(both_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(both_acc)\n",
    "        acc.append((sub,yesno_acc))\n",
    "        acc.append((sub,reason_acc))\n",
    "        acc.append((sub,both_acc))\n",
    "        \n",
    "        write_text_file(f'{folder_name}/detective.txt',f'-----{sub} end-----')\n",
    "        write_text_file(f'{folder_name}/detective.txt',yesno_acc)\n",
    "        write_text_file(f'{folder_name}/detective.txt',reason_acc)\n",
    "        write_text_file(f'{folder_name}/detective.txt',both_acc)\n",
    "        \n",
    "        \n",
    "        \"\"\" „ÇØ„Ç®„É™ÔºöËâØÂìÅ \"\"\"\n",
    "        sentence = f\"{sub} --> {gt}\"\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',sentence)\n",
    "        if order: # demo_image_one: ËâØÂìÅ, demo_image_two: ‰∏çËâØÂìÅ\n",
    "            sentence = f\"context1: OK, context2: NG, query: OK\"\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',sentence)\n",
    "        \n",
    "        else: # demo_image_one: ‰∏çËâØÂìÅ, demo_image_two: ËâØÂìÅ\n",
    "            sentence = f\"context1: NG, context2: OK, query: OK\"\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',sentence)\n",
    "        \n",
    "        write_text_file(f'{folder_name}/non-detective.txt',f'-----{sub} start-----')\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',\"\")\n",
    "            \n",
    "        query_folder_path = f\"/data/dataset/mvtec/{folder}/test/good\"\n",
    "        query_image_paths = get_image_paths(query_folder_path)\n",
    "        yesno_count = 0\n",
    "        reason_count = 0\n",
    "        both_count = 0\n",
    "        for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "            # print(query_image_path)\n",
    "            query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "            vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "            # Get the data type from model's parameters\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "            # Convert tensors to the model's data type\n",
    "            vision_x = vision_x.to(dtype=model_dtype)\n",
    "            lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "            lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "            bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x.to(model.device),\n",
    "                lang_x=lang_x_input_ids.to(model.device),\n",
    "                attention_mask=lang_x_attention_mask.to(model.device),\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                no_repeat_ngram_size=3,\n",
    "                bad_words_ids=bad_words_id,\n",
    "            )\n",
    "\n",
    "            parsed_output = (\n",
    "                model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "            )\n",
    "            \n",
    "            print(inputs)\n",
    "            print(parsed_output)\n",
    "            \n",
    "            if parsed_output.split(\" \")[0].lower()==\"no\":\n",
    "                yesno_count += 1\n",
    "                if len(parsed_output.split(\" \")) > 1:\n",
    "                    if parsed_output.split(\" \")[1].lower()==\"none\":\n",
    "                        both_count += 1\n",
    "            if len(parsed_output.split(\" \")) > 1:\n",
    "                if parsed_output.split(\" \")[1].lower()==\"none\":\n",
    "                        reason_count += 1\n",
    "                \n",
    "            write_text_file(f'{folder_name}/non-detective.txt',query_image_path)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',parsed_output)\n",
    "            write_text_file(f'{folder_name}/non-detective.txt',\"\")\n",
    "            \n",
    "            # print(inputs)\n",
    "            # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "            # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "            # axes[0].imshow(demo_image_one)\n",
    "            # axes[0].axis('off')\n",
    "            # axes[1].imshow(demo_image_two)\n",
    "            # axes[1].axis('off')\n",
    "            # axes[2].imshow(query_image)\n",
    "            # axes[2].axis('off')\n",
    "            # plt.show()\n",
    "            print(i,sub)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        yesno_acc = f\"correct: {yesno_count}, total: {len(query_image_paths)-1}, yesno acc: {(yesno_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(yesno_acc)\n",
    "        reason_acc = f\"correct: {reason_count}, total: {len(query_image_paths)-1}, reason acc: {(reason_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(reason_acc)\n",
    "        both_acc = f\"correct: {both_count}, total: {len(query_image_paths)-1}, both acc: {(both_count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "        # print(both_acc)\n",
    "        acc.append((\"good\",yesno_acc))\n",
    "        acc.append((\"good\",reason_acc))\n",
    "        acc.append((\"good\",both_acc))\n",
    "        \n",
    "        write_text_file(f'{folder_name}/non-detective.txt',f'-----{sub} end-----')\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',yesno_acc)\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',reason_acc)\n",
    "        write_text_file(f'{folder_name}/non-detective.txt',both_acc)\n",
    "        \n",
    "    for a in acc:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # „ÇØ„Ç®„É™„ÅÆ„Åø‰ΩøÁî®\n",
    "# def test(category, anormaly_reason, anormaly_type, model_name, order):\n",
    "#     if category==\"grid\":\n",
    "#         category__ = \"metal grid\"\n",
    "#     else:\n",
    "#         category__ = category\n",
    "#     category__ = category__.replace('_', ' ')\n",
    "#     for j, (ano_type,ano_reason) in enumerate(zip(anormaly_type,anormaly_reason)):\n",
    "#         folder_name = f'./result/{category}/{ano_type}/{model_name}'\n",
    "#         os.makedirs(folder_name, exist_ok=True)\n",
    "#         with open(f'{folder_name}/detective_1.txt', mode='w') as f:\n",
    "#             f.close()\n",
    "#         with open(f'{folder_name}/non-detective_1.txt', mode='w') as f:\n",
    "#             f.close()\n",
    "        \n",
    "#         subfolder_string = generate_list_string(anormaly_reason)\n",
    "#         model.text_tokenizer.padding_side = \"left\"\n",
    "        \n",
    "#         \"\"\" „ÇØ„Ç®„É™Ôºö‰∏çËâØÂìÅ \"\"\"\n",
    "#         sentence = f\"query: NG\"\n",
    "#         # print(sentence)\n",
    "#         write_text_file(f'{folder_name}/detective_1.txt',sentence)\n",
    "#         # long\n",
    "#         inputs = textwrap.dedent(f\"\"\"\n",
    "#            <image>User: This is an image of {category__}. Does this wood have any defects such as {subfolder_string}? GPT:<answer>\n",
    "#         \"\"\")\n",
    "#         # short\n",
    "#         # inputs = textwrap.dedent(f\"\"\"\n",
    "#         #     <image>User: This is an image of {category__}. Does this wood have any defects? GPT:<answer>\n",
    "#         # \"\"\")        \n",
    "        \n",
    "#         inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "#         lang_x = model.text_tokenizer(\n",
    "#             [\n",
    "#                 inputs\n",
    "#             ],\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "        \n",
    "#         write_text_file(f'{folder_name}/detective_1.txt',f'-----{ano_type} start-----')\n",
    "#         write_text_file(f'{folder_name}/detective_1.txt',\"\")\n",
    "            \n",
    "#         query_folder_path = f\"/data/dataset/mvtec/{category}/test/{ano_type}\"\n",
    "#         query_image_paths = get_image_paths(query_folder_path)\n",
    "#         count = 0\n",
    "#         for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "#             # print(query_image_path)\n",
    "#             query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "#             vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "        \n",
    "#             # Get the data type from model's parameters\n",
    "#             model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "#             # Convert tensors to the model's data type\n",
    "#             vision_x = vision_x.to(dtype=model_dtype)\n",
    "#             lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "#             lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "#             bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "#             generated_text = model.generate(\n",
    "#                 vision_x=vision_x.to(model.device),\n",
    "#                 lang_x=lang_x_input_ids.to(model.device),\n",
    "#                 attention_mask=lang_x_attention_mask.to(model.device),\n",
    "#                 max_new_tokens=512,\n",
    "#                 num_beams=3,\n",
    "#                 no_repeat_ngram_size=3,\n",
    "#                 bad_words_ids=bad_words_id,\n",
    "#             )\n",
    "\n",
    "#             parsed_output = (\n",
    "#                 model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "#             )\n",
    "            \n",
    "#             if parsed_output.split(\".\")[0].lower()==\"yes\":\n",
    "#                 count += 1\n",
    "            \n",
    "#             write_text_file(f'{folder_name}/detective_1.txt',query_image_path)\n",
    "#             write_text_file(f'{folder_name}/detective_1.txt',parsed_output)\n",
    "#             write_text_file(f'{folder_name}/detective_1.txt',\"\")\n",
    "            \n",
    "#             # print(inputs)\n",
    "#             # print(\"GPT:\", parsed_output)\n",
    "            \n",
    "#             # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "#             # axes[0].imshow(demo_image_one)\n",
    "#             # axes[0].axis('off')\n",
    "#             # axes[1].imshow(demo_image_two)\n",
    "#             # axes[1].axis('off')\n",
    "#             # axes[2].imshow(query_image)\n",
    "#             # axes[2].axis('off')\n",
    "#             # plt.show()\n",
    "            \n",
    "#         acc = f\"correct: {count}, total: {len(query_image_paths)-1}, acc: {(count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "#         print(acc)\n",
    "        \n",
    "#         write_text_file(f'{folder_name}/detective_1.txt',f'-----{ano_type} end-----')\n",
    "#         write_text_file(f'{folder_name}/detective_1.txt',acc)\n",
    "        \n",
    "#         \"\"\" „ÇØ„Ç®„É™ÔºöËâØÂìÅ \"\"\"\n",
    "#         if j==0:\n",
    "#             sentence = f\"query: OK\"\n",
    "#             # print(sentence)\n",
    "#             write_text_file(f'{folder_name}/non-detective_1.txt',sentence)\n",
    "#             write_text_file(f'{folder_name}/non-detective_1.txt',f'-----{ano_type} start-----')\n",
    "#             write_text_file(f'{folder_name}/non-detective_1.txt',\"\")\n",
    "                \n",
    "#             query_folder_path = f\"/data/dataset/mvtec/{category}/test/good\"\n",
    "#             query_image_paths = get_image_paths(query_folder_path)\n",
    "#             count = 0\n",
    "#             for i, query_image_path in enumerate(query_image_paths[1:]):\n",
    "#                 # print(query_image_path)\n",
    "#                 query_image = Image.open(query_image_path).resize((224, 224)).convert(\"RGB\")\n",
    "#                 vision_x = image_processor.preprocess([query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "            \n",
    "#                 # Get the data type from model's parameters\n",
    "#                 model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "#                 # Convert tensors to the model's data type\n",
    "#                 vision_x = vision_x.to(dtype=model_dtype)\n",
    "#                 lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "#                 lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "#                 bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "#                 generated_text = model.generate(\n",
    "#                     vision_x=vision_x.to(model.device),\n",
    "#                     lang_x=lang_x_input_ids.to(model.device),\n",
    "#                     attention_mask=lang_x_attention_mask.to(model.device),\n",
    "#                     max_new_tokens=512,\n",
    "#                     num_beams=3,\n",
    "#                     no_repeat_ngram_size=3,\n",
    "#                     bad_words_ids=bad_words_id,\n",
    "#                 )\n",
    "\n",
    "#                 parsed_output = (\n",
    "#                     model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "#                 )\n",
    "                \n",
    "#                 if parsed_output.split(\".\")[0].lower()==\"no\":\n",
    "#                     count += 1\n",
    "                \n",
    "#                 write_text_file(f'{folder_name}/non-detective_1.txt',query_image_path)\n",
    "#                 write_text_file(f'{folder_name}/non-detective_1.txt',parsed_output)\n",
    "#                 write_text_file(f'{folder_name}/non-detective_1.txt',\"\")\n",
    "                \n",
    "#                 # print(inputs)\n",
    "#                 # print(\"GPT:\", parsed_output)\n",
    "                \n",
    "#                 # fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "#                 # axes[0].imshow(demo_image_one)\n",
    "#                 # axes[0].axis('off')\n",
    "#                 # axes[1].imshow(demo_image_two)\n",
    "#                 # axes[1].axis('off')\n",
    "#                 # axes[2].imshow(query_image)\n",
    "#                 # axes[2].axis('off')\n",
    "#                 # plt.show()\n",
    "                \n",
    "#             acc = f\"correct: {count}, total: {len(query_image_paths)-1}, acc: {(count / (len(query_image_paths)-1)) * 100:.2f}%\"\n",
    "#             print(acc)\n",
    "            \n",
    "#             write_text_file(f'{folder_name}/non-detective_1.txt',f'-----{ano_type} end-----')\n",
    "#             write_text_file(f'{folder_name}/non-detective_1.txt',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('broken_large', 'correct: 18, total: 19, yesno acc: 94.74%')\n",
      "('broken_large', 'correct: 0, total: 19, reason acc: 0.00%')\n",
      "('broken_large', 'correct: 0, total: 19, both acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, yesno acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, reason acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, both acc: 0.00%')\n",
      "('contamination', 'correct: 17, total: 20, yesno acc: 85.00%')\n",
      "('contamination', 'correct: 0, total: 20, reason acc: 0.00%')\n",
      "('contamination', 'correct: 0, total: 20, both acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, yesno acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, reason acc: 0.00%')\n",
      "('good', 'correct: 0, total: 19, both acc: 0.00%')\n"
     ]
    }
   ],
   "source": [
    "folder = \"bottle\"\n",
    "sub_folder = [\"broken_large\",\"contamination\"]\n",
    "GTs = ['broken','contamination']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "# sub_folder = [\"broken_small\",\"contamination\"]\n",
    "# test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"cable\"\n",
    "sub_folder = [\"bent_wire\",\"cable_swap\",\"cut_inner_insulation\",\"missing_cable\",\"poke_insulation\"]\n",
    "GTs = ['bent','swapp','crack','missing','hole']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"bent_wire\",\"cable_swap\",\"cut_outer_insulation\",\"missing_wire\",\"poke_insulation\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"capsule\"\n",
    "sub_folder = [\"crack\",\"faulty_imprint\",\"poke\",\"scratch\",\"squeeze\"]\n",
    "GTs = ['crack','misprint','hole','scratch','misshapen']\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"carpet\"\n",
    "sub_folder = [\"color\",\"cut\",\"hole\",\"metal_contamination\"]\n",
    "GTs = ['stain','cut','hole','contamination']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"color\",\"cut\",\"hole\",\"thread\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"grid\"\n",
    "sub_folder = [\"bent\",\"broken\",\"glue\"]\n",
    "GTs = [\"bent\",\"broken\",\"contamination\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"bent\",\"broken\",\"metal_contamination\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"bent\",\"broken\",\"thread\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"hazelnut\"\n",
    "sub_folder = [\"crack\",\"cut\",\"hole\",\"print\"]\n",
    "GTs = ['crack','scratch','hole','misprint']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"leather\"\n",
    "sub_folder = [\"color\",\"cut\",\"fold\",\"poke\"]\n",
    "GTs = ['stain','scratch','wrinkle','hole']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"glue\",\"cut\",\"fold\",\"poke\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"glue\",\"poke\",\"fold\",\"cut\"]\n",
    "GTs = ['stain','hole','wrinkle','scratch']\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"metal_nut\"\n",
    "sub_folder = [\"bent\",\"color\",\"flip\",\"scratch\"]\n",
    "GTs = ['bent','stain','flip','scratch']\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"pill\"\n",
    "sub_folder = [\"color\",\"contamination\",\"crack\",\"faulty_imprint\",\"scratch\"]\n",
    "GTs = ['stain','contamination','crack','misprint','scratch']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"pill_type\",\"contamination\",\"crack\",\"faulty_imprint\",\"scratch\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"screw\"\n",
    "sub_folder = [\"scratch_head\",\"manipulated_front\"]\n",
    "GTs = ['chip','strip']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"scratch_neck\",\"manipulated_front\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"thread_side\",\"manipulated_front\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"thread_top\",\"manipulated_front\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"tile\"\n",
    "sub_folder = [\"crack\",\"glue_strip\",\"gray_stroke\"]\n",
    "GTs = ['crack','contamination','stain']\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"crack\",\"rough\",\"oil\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"toothbrush\"\n",
    "sub_folder = [\"defective\"]\n",
    "GTs = [\"broken\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"transistor\"\n",
    "sub_folder = [\"bent_lead\",\"cut_lead\",\"damaged_case\",\"misplaced\"]\n",
    "GTs = [\"bent\",\"cut\",\"broken\",\"misalignment\"]\n",
    "\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"wood\"\n",
    "sub_folder = [\"color\",\"scratch\",\"hole\"]\n",
    "GTs = [\"stain\",\"scratch\",\"hole\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"liquid\",\"scratch\",\"hole\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"zipper\"\n",
    "sub_folder = [\"broken_teeth\",\"fabric_border\",\"fabric_interior\",\"split_teeth\"]\n",
    "GTs = [\"broken\",\"tear\",\"frayed\",\"misshapen\"]\n",
    "test(folder, sub_folder, GTs, model_name)\n",
    "\n",
    "sub_folder = [\"broken_teeth\",\"fabric_border\",\"rough\",\"squeezed_teeth\"]\n",
    "test(folder, sub_folder, GTs, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImagenetÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MI_loss_context_query_debug/batch128_epoch1_lr-5_pairs4\"\n",
    "trained_ckpt_path = f'../../log/{model_name}/final_weights.pt'\n",
    "\n",
    "train_ckpt = torch.load(trained_ckpt_path, map_location=\"cpu\")\n",
    "if train_ckpt.get(\"model_state_dict\", None) is not None:\n",
    "    train_ckpt = train_ckpt[\"model_state_dict\"]\n",
    "_ = model.load_state_dict(train_ckpt, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import orjson\n",
    "\n",
    "print(\"train\")\n",
    "images = {}\n",
    "images_path=\"/data/dataset/MIMIC-IT/MiniImagenet_jsons/MI_train.json\"\n",
    "with open(images_path, \"rb\") as f:\n",
    "    for key, value in ijson.kvitems(f, \"\", use_float=True):\n",
    "        images[key] = value\n",
    "\n",
    "train_config_path=\"/data/dataset/MIMIC-IT/MiniImagenet_jsons/MI_train_pairs4_train.json\"\n",
    "with open(train_config_path, \"rb\") as f:\n",
    "    cache_train_config = orjson.loads(f.read())\n",
    "\n",
    "mimicit_path=\"/data/dataset/MIMIC-IT/MiniImagenet_jsons/MI_train_instructions.json\"\n",
    "with open(mimicit_path, \"rb\") as f:\n",
    "    instructions = orjson.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ê≠£Ëß£Áéá\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "count = 0\n",
    "NUM = 100\n",
    "for i in range(len(keys[:NUM])):\n",
    "    print(i)\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "    \n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "    \n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ÄÈÉ®ÂèØË¶ñÂåñ\n",
    "import random\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "keys = list(cache_train_config.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "NUM = 50\n",
    "count = 0\n",
    "for i in range(len(keys[:NUM])):\n",
    "    context1 = cache_train_config[keys[i]][0]\n",
    "    context2 = cache_train_config[keys[i]][1]\n",
    "    query = keys[i].split('=')[0]\n",
    "\n",
    "    str_data1 = images[context1] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà1\n",
    "    str_data2 = images[context2] # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà2\n",
    "    str_data3 = images[query] # „ÇØ„Ç®„É™\n",
    "\n",
    "    # „Éá„Ç≥„Éº„Éâ„Åó„Åü„Éê„Ç§„Éà„Éá„Éº„Çø„ÇíImage„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ\n",
    "    demo_image_one = Image.open(BytesIO(base64.urlsafe_b64decode(str_data1))).convert(\"RGB\")\n",
    "    demo_image_two = Image.open(BytesIO(base64.urlsafe_b64decode(str_data2))).convert(\"RGB\")\n",
    "    query_image = Image.open(BytesIO(base64.urlsafe_b64decode(str_data3))).convert(\"RGB\")\n",
    "\n",
    "    vision_x = image_processor.preprocess([demo_image_one, demo_image_two, query_image], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    model.text_tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = textwrap.dedent(f\"\"\"\n",
    "        <image>User:{instructions[\"data\"][context1][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context1][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][context2][\"instruction\"]} GPT:<answer>{instructions[\"data\"][context2][\"answer\"]}<|endofchunk|>\n",
    "        <image>User:{instructions[\"data\"][query][\"instruction\"]} GPT:<answer>\n",
    "    \"\"\")\n",
    "    inputs = \"\".join(inputs.split(\"\\n\"))\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            inputs\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"]\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"]\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n",
    "    )\n",
    "    \n",
    "    if instructions[\"data\"][query][\"answer\"].lower()==parsed_output.lower():\n",
    "        count += 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "    axes[0].imshow(demo_image_one)\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(demo_image_two)\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(query_image)\n",
    "    axes[2].axis('off')\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(query)\n",
    "    print(inputs)\n",
    "    print(\"GPT:\", parsed_output)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"correct: {count}, total: {NUM}, acc: {(count / NUM) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
